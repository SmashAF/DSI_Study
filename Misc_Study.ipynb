{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f589b74b5f9dfadb2b978c0f6d0c86213b22d6e1bb02d62665d77952f7aaa902"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def fiz_buzz():\n",
    "    lst = range(1, 100+1)    \n",
    "    out = []\n",
    "    for num in lst:\n",
    "        if num % 3 == 0 and num % 5 == 0:\n",
    "            out.append('FizzBuzz')\n",
    "        elif num % 3 == 0: \n",
    "            out.append('Fizz')\n",
    "        elif num % 5 ==0: \n",
    "            out.append('Buzz')\n",
    "        else: out.append(num)\n",
    "    return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def count_characters(string):\n",
    "    '''\n",
    "    Return a dictionary which contains\n",
    "    a count of the number of times each character appears in the string.\n",
    "    Characters which with a count of 0 should not be included in the\n",
    "    output dictionary.\n",
    "\n",
    "    '''\n",
    "    d = dict()\n",
    "  \n",
    "    for char in string:\n",
    "        if char not in d:\n",
    "            d[char] = 1\n",
    "        else: \n",
    "            d[char] += 1\n",
    "    return d"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def merge_dictionaries(d1, d2):\n",
    "        d3 = d2.copy()\n",
    "    for k, v in d1.items():\n",
    "        if k not in d3:\n",
    "            d3[k] = v\n",
    "        else: \n",
    "            d3[k]+= v\n",
    "    return d3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def cookie_jar(a, b):\n",
    "    '''\n",
    "    There are two jars of cookies.\n",
    "    Each has chocolate and peanut butter cookies.\n",
    "    Input 'a' is the fraction of cookies in Jar A which are chocolate.\n",
    "    Input 'b' is the fraction of cookies in Jar B which are chocolate.\n",
    "    A jar is chosen at random and a cookie is drawn.\n",
    "    The cookie is chocolate.\n",
    "    Return the probability that the cookie came from Jar A.\n",
    "    '''\n",
    "       return a / (a + b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import numpy as np\n",
    "\n",
    "def boolean_indexing(arr, minimum):\n",
    "    '''Return an array of only the elements of \"arr\" that are greater than\n",
    "    or equal to \"minimum\".\n",
    "\n",
    "    FOR FULL POINTS DON'T USE LOOPS OR LIST COMPREHENSIONS.\n",
    "\n",
    "    >>>boolean_indexing(np.array([[3, 4, 5], [6, 7, 8]]), 7)\n",
    "    array([7, 8])\n",
    "    '''\n",
    "    return arr[arr >= minimum]\n",
    "\n",
    "print(boolean_indexing(np.array([[3, 4, 5], [6, 7, 8]]), 7))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[7 8]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def size_of_multiply(A, B):\n",
    "    '''If matrices A (dimensions m x n) and B (dimensions p x q) can be\n",
    "    multiplied (AB), returns the shape of the result of multiplying them.\n",
    "    Function DOES NOT perform the multiplication. If A and B cannot be\n",
    "    multiplied, returns None. (Use the shape function.)\n",
    "\n",
    "    DO NOT PERFORM ANY MATRIX MULTIPLICATION IN YOUR FUNCTION.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (int, int) or None  \n",
    "    '''\n",
    "    if A.shape[1] == B.shape[0]:\n",
    "        return(A@B).shape\n",
    "    else:\n",
    "        return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def data_frame_work(df, colA, colB, colC):\n",
    "    '''\n",
    "    Insert a column (colC) into the dataframe that is the sum of\n",
    "    colA and colB. Assume that df contains columns colA and colB and\n",
    "    that these are numeric.\n",
    "    '''\n",
    "    df[colC] = df[colA] + df[colB]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def max_lists(list1, list2):\n",
    "    '''\n",
    "    list1 and list2 have the same length. Return a list which contains \n",
    "    the maximum element of each list for every index.\n",
    "    '''\n",
    "    return [max(i1, i2) for i1, i2 in zip(list1, list2)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def invert_dictionary(d):\n",
    "    '''\n",
    "    Given a dictionary d, return a new dictionary with d's values as \n",
    "    keys and the value for a given key being the set of d's keys which \n",
    "    shared the same value.\n",
    "    e.g. {'a': 2, 'b': 4, 'c': 2} => {2: {'a', 'c'}, 4: {'b'}}\n",
    "    '''\n",
    "    result = {}\n",
    "    for key, value in d.items():\n",
    "        if value not in result:\n",
    "            # We use a set since original keys had no duplicates\n",
    "            result[value] = set()\n",
    "        # We can now safely call the .add method\n",
    "        result[value].add(key)\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def matrix_multiplication(A, B):\n",
    "    '''\n",
    "    Return the matrix which is the product of matrix A and matrix B \n",
    "    where A and B will be (a) integer valued (b) square matrices (c) of \n",
    "    size n-by-n (d) encoded as lists of lists,  e.g.\n",
    "    A = [[2, 3, 4], [6, 4, 2], [-1, 2, 0]] corresponds to the matrix\n",
    "    | 2  3  4 |\n",
    "    | 6  4  2 |\n",
    "    |-1  2  0 |\n",
    "    \n",
    "    YOU MAY NOT USE NUMPY. Write your solution in straight python.\n",
    "    '''\n",
    "\n",
    "    n = len(A)\n",
    "    result = []\n",
    "    # iterate over the rows of A\n",
    "    for i in range(n):\n",
    "        row = []\n",
    "        # iterate over the columns of B\n",
    "        for j in range(n):\n",
    "            total = 0\n",
    "            # iterate ith row of A with jth column of B dot product\n",
    "            for k in range(n):\n",
    "                # k implements [ith row][jth column] element-wise dot product\n",
    "                total += A[i][k] * B[k][j]\n",
    "            # column j of row i\n",
    "            row.append(total)\n",
    "        # all columns j of row i completed\n",
    "        result.append(row)\n",
    "    # all rows done\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def reverse_index(arr, finRow, finCol):\n",
    "    '''\n",
    "    Reverse the row order of \"arr\" (i.e. so the top row is on the \n",
    "    bottom) and return the sub-matrix from coordinate [0, 0] up to \n",
    "    (but not including) [finRow, finCol].\n",
    "\n",
    "    Ex:\n",
    "    In [1]: arr = np.array([[ -4,  -3,  11],\n",
    "                            [ 14,   2, -11],\n",
    "                            [-17,  10,   3]])\n",
    "    In [2]: reverse_index(arr, 2, 2)\n",
    "    Out[2]:\n",
    "    array([[-17, 10],\n",
    "           [ 14,  2]])\n",
    "\n",
    "    Hint: this can be using two steps of slicing that can be combined \n",
    "    into a one-liner.\n",
    "    '''\n",
    "    return arr[::-1][:finRow, :finCol]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "def array_work(rows, cols, scalar, matrixA):\n",
    "    '''\n",
    "    (of matrix product of r-by-c matrix of \"scalar\"'s time matrixA)\n",
    "\n",
    "    Create matrix of size (rows, cols) with elements initialized to the \n",
    "    scalar value. Right multiply that matrix with the passed matrixA \n",
    "    (i.e. AB, not BA).  Return the result of the multiplication. You \n",
    "    needn't check for matrix compatibililty, but you accomplish this in \n",
    "    a single line.\n",
    "\n",
    "\n",
    "    E.g., array_work(2, 3, 5, np.array([[3, 4], [5, 6], [7, 8]]))\n",
    "           [[3, 4],      [[5, 5, 5],\n",
    "            [5, 6],   *   [5, 5, 5]]\n",
    "            [7, 8]]\n",
    "    '''\n",
    "    return matrixA.dot(np.ones((rows, cols)) * scalar)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def pandas_add_increase_column(df):\n",
    "    '''\n",
    "    Add a column to the DataFrame called 'Increase' which contains the \n",
    "    amount that the median rent increased by from 2011 to 2014.\n",
    "    '''\n",
    "    df['Increase'] = df['med_2014'] - df['med_2011']\n",
    "    # Another solution:\n",
    "    # df.eval('Increase = med_2014 - med_2011', inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def pandas_max_rent(df):\n",
    "    '''\n",
    "    Return a new pandas DataFrame which contains every city and the \n",
    "    highest median rent from that city for 2011 and 2014.\n",
    "    Your DataFrame should contain these columns:\n",
    "        City, State, med_2011, med_2014\n",
    "    '''\n",
    "    return df[['City', 'State', 'med_2011', 'med_2014']].groupby(['City', 'State']).max()\n",
    "\n",
    "    # Another solution:\n",
    "    # return df.groupby(['City', 'State']).max().reset_index()[['City', 'State', 'med_2011', 'med_2014']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from numpy import random\n",
    "import numpy as np\n",
    "def roll_the_dice(n_simulations = 1000):\n",
    "    '''\n",
    "    two unbiased, six sided, dice are thrown once and the sum of the \n",
    "    showing faces is observed (so if you rolled a 3 and a 1, you would \n",
    "    observe the sum, 4). use a simulation to find the estimated \n",
    "    probability that the total score is an even number or a number \n",
    "    greater than 7.  your function should return an estimated \n",
    "    probability, based on rolling the two dice n_simulations times.\n",
    "    '''\n",
    "    total = 0\n",
    "    num_repeats = 10000\n",
    "    for i in range(num_repeats):\n",
    "        die1 = random.randint(1, 6+1)\n",
    "        die2 = random.randint(1, 6+1)\n",
    "        score = die1 + die2\n",
    "        if score % 2 == 0 or score > 7:\n",
    "            total += 1\n",
    "    return float(total) / num_repeats\n",
    "    # with numpy operations\n",
    "    # two_dice_sum = np.random.randint(1, 7, (2, 10000)).sum(axis=0)\n",
    "    # return np.logical_or(two_dice_sum > 7, np.logical_not(two_dice_sum % 2)).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "def return_percentage_greater_than_80(arr):\n",
    "  '''\n",
    "  Return the percentage of the rows in arr where the sum of the two values is greater than 80. \n",
    "  *You should use numpy to do this.* \n",
    "  '''\n",
    "  return sum(np.sum(arr, axis = 1) > 80)/len(arr)*100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "import numpy as np\n",
    "def only_positive(arr):\n",
    "    '''\n",
    "    Return a numpy array containing only the rows from arr where all \n",
    "    the values in that row are positive.\n",
    "\n",
    "    E.g.  np.array([[1, -1, 2], \n",
    "                    [3, 4, 2], \n",
    "                    [-8, 4, -4]])\n",
    "              ->  np.array([[3, 4, 2]])\n",
    "\n",
    "    Use numpy methods to do this, full credit will not be awarded for a \n",
    "    python for loop.\n",
    "    '''\n",
    "    return arr[np.min(arr, 1) > 0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "def pandas_query(df):\n",
    "    '''\n",
    "    input: dataframe\n",
    "    output: dataframe\n",
    "\n",
    "    given a dataframe containing university data with these columns:\n",
    "        name, address, website, type, size\n",
    "\n",
    "    return the dataframe containing the average size for each \n",
    "    university type ordered by average size in ascending order.\n",
    "    '''\n",
    "    return df.groupby('Type').mean().sort_values(by='Size')\n",
    "    # alternative:\n",
    "    # return df.groupby(\"Type\")[\"Size\"].mean().sort_values()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "def return_loans_by_id(df):\n",
    "    '''\n",
    "    Takes a df and returns a df for loans with the IDs '7484', '4423', and '9910'.\n",
    "    The dataframe’s index has been set to be the loan IDs (a string). \n",
    "    '''\n",
    "    return df.loc[['7484', '4423', '9910']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "import pandas as pd\n",
    "def return_loans(df):\n",
    "    '''\n",
    "    Takes a df and returns data at the 0th, 1st, and 4th indices.\n",
    "    '''\n",
    "    return df.iloc[[0, 1, 4]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def divisible_by(arr, int1, int2):\n",
    "    '''\n",
    "    arr in a numpy array of integers.  int1 and int2 are integers. \n",
    "    Return an array of the integers in arr that are divisible without \n",
    "    remainder by both int1 and int2. \n",
    "\n",
    "    For example:\n",
    "    In [1] arr_out = divisible_by(np.array([0, 24, 3, 12, 18, 17]), 3, 4)\n",
    "    In [2] arr_out\n",
    "    Out[2] np.array([0, 24, 12])\n",
    "    '''\n",
    "    return arr[(arr%int1==0) & (arr%int2==0)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def bubble_sort(arr):\n",
    "    '''\n",
    "    Implement the bubble sort algorithm to sort arr.  However, upon on \n",
    "    each swap, append the new list as the next row in a list \n",
    "    containing all the swaps. The original list should be the first row \n",
    "    in the list detailing the swaps.\n",
    "\n",
    "    For example:\n",
    "    In [1] arr = [7, 4, 2, 5]\n",
    "    In [2] bs = bubble_sort(arr)\n",
    "    In [3] bs\n",
    "    Out[3]\n",
    "    [[7, 4, 2, 5],\n",
    "     [4, 7, 2, 5],\n",
    "     [4, 2, 7, 5],\n",
    "     [4, 2, 5, 7],\n",
    "     [2, 4, 5, 7]]\n",
    "    ''' \n",
    "    arr_lst = []\n",
    "    arr_lst.append(arr.copy())\n",
    "    for i in range(len(arr), 0, -1):\n",
    "        for j in range(1, i):\n",
    "            if arr[j - 1] > arr[j]:\n",
    "                temp = arr[j-1]\n",
    "                arr[j-1] = arr[j]\n",
    "                arr[j] = temp\n",
    "                arr_lst.append(arr.copy())\n",
    "    return(arr_lst)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import scipy.stats as stats\n",
    "def calculate_t_test(sample1, sample2, type_i_error_rate):\n",
    "    '''\n",
    "    you are asked to evaluate whether the two samples come from a \n",
    "    population with the same population mean.  return a tuple \n",
    "    containing the p-value for the pair of samples and true or false \n",
    "    depending if the p-value is considered significant at the provided \n",
    "    type i error rate (i.e. false positive rate, i.e. alpha).\n",
    "    '''\n",
    "    _, pvalue = stats.ttest_ind(sample1, sample2)\n",
    "    return pvalue, pvalue < type_i_error_rate"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "def linear_regression(X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    Fit a linear regression model with X_train and y_train using\n",
    "    scikit-learn, and return the beta coefficients. Then calculate and\n",
    "    return the R^2 value using X_test and y_test.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    The R^2 statistic, also known as the coefficient of determination, is a\n",
    "    popular measure of fit for a linear regression model.  If you need a\n",
    "    refresher, this wikipedia page should help:\n",
    "    https://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "    '''\n",
    "    regr = LinearRegression()\n",
    "    regr.fit(X_train, y_train)\n",
    "    return regr.coef_, regr.score(X_test, y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sum_to_target(nums, target):\n",
    "    '''\n",
    "    Given an array of integers and a target integer, return \n",
    "    indices of the two numbers in the array that sum to equal \n",
    "    the target.\n",
    "\n",
    "    Return the indices in ascending order.   \n",
    "    For full points your solution should have a runtime of O(N). \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nums: list\n",
    "    target: int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "    \n",
    "    Input: nums = [3,2,4], target = 6\n",
    "    Output: [1,2]\n",
    "    '''\n",
    "    # Need to keep track of the index of numbers we've seen. \n",
    "    index = {}\n",
    "    for i,  num in enumerate(nums):\n",
    "        looking_for = target - num\n",
    "        if looking_for in index:\n",
    "            return [index[looking_for], i]\n",
    "        else:\n",
    "            index[num] = i\n",
    "\n",
    "\n",
    "    #my solution\n",
    "    work={} # use dict to hash \n",
    "    for i, n in enumerate(nums):\n",
    "        find = target - n\n",
    "        if find in work:\n",
    "            return[work[target-n], i]\n",
    "        work[n] = i"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def split_node(X, y, col, split_val):\n",
    "    '''\n",
    "    INPUT: NUMPY ARRAY, NUMPY ARRAY, INT, FLOAT\n",
    "    OUTPUT: NUMPY ARRAY, NUMPY ARRAY, NUMPY ARRAY, NUMPY ARRAY\n",
    "\n",
    "    Split a feature matrix X and the target array y into \"left\" and \"right\"\n",
    "    arrays determined by splitting on \"split_val\" in col of the X matrix.\n",
    "    The \"left\" matrices of X and y contain the rows corresponding to values in \n",
    "    col <= split value, while the \"right\" matrices contain the rows where\n",
    "    col > split value.  Return the four arrays separately (see example\n",
    "    below).\n",
    "\n",
    "    You can assume that all columns in X are continuous values.  col is\n",
    "    an integer (0 indexed) that indicates which column of X to use to split\n",
    "    the X and y arrays.\n",
    "\n",
    "    Return empty arrays for the left or right arrays if no values are returned.\n",
    "    \n",
    "    Ex:\n",
    "    In [1]: X = np.array([[ 5.5,  2.4,  3.7],\n",
    "                          [ 5.5,  2.3,  3.8],\n",
    "                          [ 6.1,  3.0,  4.9],\n",
    "                          [ 5.2,  3.5,  1.5],\n",
    "                          [ 5.7,  2.6,  3.5]])\n",
    "    \n",
    "    In [2]: y = np.array([1, 1, 2, 0, 1])\n",
    "    \n",
    "    In [3]: X_left, y_left, X_right, y_right = split_node(X, y, 1, 2.6)\n",
    "\n",
    "    In [4]: X_left\n",
    "    Out[4]:\n",
    "    array([[5.5, 2.4, 3.7],\n",
    "           [5.5, 2.3, 3.8],\n",
    "           [5.7, 2.6, 3.5]])\n",
    "    \n",
    "    In [5]: y_left\n",
    "    Out[5]: array([1, 1, 1])\n",
    "\n",
    "    '''\n",
    "    left = X[:, col] <= split_val\n",
    "    right = ~left\n",
    "    return X[left], y[left], X[right], y[right]\n",
    "\n",
    "\n",
    "def split_node(X, y, col, split_val):\n",
    "   \n",
    "    left = X[:,col] <= split_val\n",
    "    right = X[:,col] > split_val\n",
    "\n",
    "    split = X[left], y[left], X[right], y[right]\n",
    "\n",
    "    return split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "def calculate_entropy(arr):\n",
    "    '''\n",
    "    INPUT: NUMPY ARRAY of binary values (0 and 1)\n",
    "    OUTPUT: FLOAT\n",
    "\n",
    "    Return the Shannon entropy of a numpy array containing only two classes\n",
    "    (integers 0 and 1).\n",
    "\n",
    "    You can assume that the array will always contain one or more values.\n",
    "    '''\n",
    "    # solution is for an arbitrary number of classes \n",
    "    num_val = len(arr) \n",
    "    classes = np.unique(arr)\n",
    "    probs = [len(arr[arr==cls])/num_val for cls in classes]\n",
    "    return -1*sum([p*np.log2(p) for p in probs])\n",
    "    \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np    \n",
    "import pandas as pd\n",
    "def make_series(start, length, index):\n",
    "    '''\n",
    "    INPUTS: INT, INT, LIST (of length \"length\")\n",
    "    OUTPUT: PANDAS SERIES (of \"length\" sequential integers\n",
    "             beginning with \"start\" and with index \"index\")\n",
    "\n",
    "    Create a pandas Series of length \"length\" with index \"index\"\n",
    "    and with elements that are sequential integers starting from \"start\".\n",
    "    You may assume the length of index will be \"length\".\n",
    "\n",
    "    E.g.,\n",
    "    In [1]: make_series(5, 3, ['a', 'b', 'c'])\n",
    "    Out[1]:\n",
    "    a    5\n",
    "    b    6\n",
    "    c    7\n",
    "    dtype: int64\n",
    "    '''\n",
    "    return pd.Series(np.arange(length) + start, index=index)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sql_query():\n",
    "    '''\n",
    "    given a table named universities which contains university data \n",
    "    with these columns:\n",
    "\n",
    "        name, address, website, type, size\n",
    "\n",
    "    return a sql query that gives the average size of each university \n",
    "    type in ascending order.\n",
    "    '''\n",
    "    # your code should look like this:\n",
    "    # return '''select * from universities;'''\n",
    "    return '''select \n",
    "                type, \n",
    "                avg(size) as avg_size \n",
    "              from universities \n",
    "              group by type \n",
    "              order by avg_size;\n",
    "           '''\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def markets_per_state():\n",
    "    '''\n",
    "    Return a SQL statement which gives the states and the number of \n",
    "    markets for each state which take WIC or WICcash.\n",
    "    '''\n",
    "\n",
    "    return '''SELECT State, COUNT(1)\n",
    "              FROM farmersmarkets\n",
    "              WHERE WIC='Y' OR WICcash='Y'\n",
    "              GROUP BY State;'''\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sql_count_neighborhoods():\n",
    "    '''\n",
    "    Return a SQL query that gives the number of neighborhoods in each city\n",
    "    according to the rent table. Keep in mind that city names are not always\n",
    "    unique unless you include the state as well, so your result should have\n",
    "    these columns: city, state, cnt\n",
    "    '''\n",
    "    return '''SELECT city, state, COUNT(1) AS cnt\n",
    "              FROM rent\n",
    "              GROUP BY city, state;'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sql_highest_rent_increase():\n",
    "    '''\n",
    "    INPUT: None\n",
    "    OUTPUT: string\n",
    "\n",
    "    Return a SQL query that gives the 5 San Francisco neighborhoods with the\n",
    "    highest rent increase.\n",
    "    '''\n",
    "    return '''SELECT neighborhood\n",
    "              FROM rent\n",
    "              WHERE city='San Francisco'\n",
    "              AND rent.med_2014 - rent.med_2011 IS NOT NULL\n",
    "              ORDER BY med_2014-med_2011 DESC LIMIT 5;'''\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sql_rent_and_buy():\n",
    "    '''\n",
    "    INPUT: None\n",
    "    OUTPUT: string\n",
    "\n",
    "    Return a SQL query that gives the rent price and buying price for 2014 for\n",
    "    all the neighborhoods in San Francisco.\n",
    "    Your result should have these columns:\n",
    "        neighborhood, rent, buy\n",
    "    '''\n",
    "    return '''SELECT a.neighborhood, a.med_2014 AS rent, b.med_2014 AS buy\n",
    "              FROM rent a\n",
    "              JOIN buy b\n",
    "              ON a.neighborhood=b.neighborhood AND a.city=b.city AND a.state=b.state\n",
    "              WHERE a.city='San Francisco';'''\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "COALESCE\n",
    "\n",
    "SELECT COALESCE(NULL,1); \n",
    "\n",
    "SELECT \n",
    "    COALESCE(NULL, 'Hi', 'Hello', NULL) result; #returns 'Hi'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "COMPARISONS\n",
    "\n",
    "SELECT 15>14 FROM dual;\n",
    "\n",
    "\n",
    "\n",
    "SELECT *\n",
    "FROM agents\n",
    "WHERE commission = 0.15;\n",
    "\n",
    "\n",
    "SELECT *\n",
    "FROM agents\n",
    "WHERE commission <> 0.15; # not equl to .15\n",
    "\n",
    "# Find city names that start with a vowel\n",
    "SELECT DISTINCT city \n",
    "FROM station \n",
    "WHERE CITY \n",
    "REGEXP '^[aeiou]';\n",
    "\n",
    "# Find city names that end with a vowel\n",
    "SELECT DISTINCT city \n",
    "FROM station \n",
    "WHERE CITY \n",
    "REGEXP '[aeiou]$';\n",
    "\n",
    "# Find customer name with name starting with ‘A’.\n",
    "SELECT CustomerName FROM Customer WHERE CustomerName LIKE 'A%';\n",
    "\n",
    "#Find customer name with name ending with ‘e’.\n",
    "SELECT CustomerName FROM Customer WHERE CustomerName LIKE '%e'\n",
    "\n",
    "#Find customer name with name starting with ‘A’ and ending with ‘t’.\n",
    "SELECT CustomerName FROM Customer WHERE CustomerName LIKE 'A%t'\n",
    "\n",
    "#Find customer name with name containing ‘n’ at any position.\n",
    "SELECT CustomerName FROM Customer WHERE CustomerName LIKE '%n%'\n",
    "\n",
    "#Find customer name with name containing ‘n’ at second position.\n",
    "SELECT CustomerName FROM Customer WHERE CustomerName LIKE '_n%'\n",
    "\n",
    "\n",
    "#Find customer name with name containing ‘i’ at third position and ending with ‘t’.\n",
    "SELECT CustomerName FROM Customer WHERE CustomerName LIKE '__i%t'\n",
    "\n",
    "\n",
    "SELECT CustomerName FROM Customer WHERE CustomerName LIKE 'Am%' OR CustomerName LIKE 'Jo%';\n",
    "\n",
    "For NOT LIKE works same \n",
    "\n",
    "\n",
    "SELECT column FROM table_name WHERE column LIKE pattern;\n",
    "\n",
    "UPDATE table_name SET column=value WHERE column LIKE pattern;\n",
    "\n",
    "DELETE FROM table_name WHERE column LIKE pattern;\n",
    "\n",
    "\n",
    "SELECT column FROM table_name WHERE column NOT LIKE pattern;\n",
    "\n",
    "UPDATE table_name SET column=value WHERE column NOT LIKE pattern;\n",
    "\n",
    "DELETE FROM table_name WHERE column NOT LIKE pattern;\n",
    "\n",
    "\n",
    "#MOC\n",
    "SELECT * \n",
    "FROM (SELECT num, company           \n",
    "FROM route\n",
    "WHERE stop = 53) AS  A\n",
    "JOIN (SELECT num, company           \n",
    "FROM route\n",
    "WHERE stop = 149) AS B\n",
    "ON A.num = B.num AND A.company = B.company\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sample mean\n",
    "\n",
    "    While it may seem strange to take the average of coin flips, when \n",
    "    the flip outcomes (heads or tails) are encoded as 1's and 0's, \n",
    "    respectively, it is of course perfectly legitimate to take the \n",
    "    average of coin flipping outcomes.  Further, this has the \n",
    "    particularly useful benefit that it captures the proportion of \n",
    "    flips that are heads (encoded as 1's). Since 1's and 0's are just \n",
    "    numbers like any other, we simply take the mean of these numbers \n",
    "    just as we would any other set of numbers. I.e., here:\n",
    "\n",
    "    ((1 + 1 + 0 + 1 + 0) / 5) = 0.6\n",
    "\n",
    "\n",
    "Probability of data given fair coin\n",
    "\n",
    "    # Solving the \"S = {1,1,0,1,0}\" problem:\n",
    "\n",
    "    The probability of this sequence of head flips (assuming a fair \n",
    "    coin) is the same as *any other* sequence of head flips, i.e., \n",
    "    (0.5)^5.\n",
    "\n",
    "    # Solving the \"3 heads\" problem:\n",
    "\n",
    "    The probability of seeing 3 heads in 5 flips (assuming a fair \n",
    "    coin), (as opposed to the sequence \"S = {1,1,0,1,0}\"), i.e., \n",
    "    Pr(3 heads in 5 flips), is determined by a binomial probability \n",
    "    model with n = 5 and p = 0.5. This model gives us the following \n",
    "    formula for the probability:\n",
    "\n",
    "    5c3 * 0.5^3 * 0.5^2 = 0.3125\n",
    "\n",
    "    5c3 is five choose three and it is equal to 5!/(3!*2!).  It is the \n",
    "    number of ways we could get 3 heads on five flips. E.g., HHHTT or \n",
    "    HTHTH, etc. And of course, the probability of a head or a tail on \n",
    "    any given flip is the same at 0.5, which shows how the binomial \n",
    "    probability distribution probabilities are produced.\n",
    "\n",
    "\n",
    "Dot product of Xy\n",
    "\n",
    "    [14 \\n 10]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Python Data Types\n",
    "\n",
    "    a) Please give an example of a mutable and immutable datatype. \n",
    "    (2 pts)\n",
    "\n",
    "    Mutable: lists.\n",
    "    Immutable: tuples.\n",
    "\n",
    "    b) What would be displayed when 'b' is printed in the code below. \n",
    "    (1 pt)\n",
    "\n",
    "    In[1] a = [1, 'a', [2, 3], 4.5]\n",
    "\n",
    "    In[2] b = a\n",
    "\n",
    "    In[3] a[0] = -0.1\n",
    "\n",
    "    In[4] print(b)\n",
    "    Out [4] [-0.1, 'a', [2, 3], 4.5]\n",
    "\n",
    "\n",
    "Git\n",
    "\n",
    "    a)  What is git, and why do software developers use it? (1 pt)\n",
    "\n",
    "    Git is a version control software. Developers use it to keep track \n",
    "    of changes in their code, work simultaneously with others on large \n",
    "    projects, and test changes in code on branches before implementing \n",
    "    (merging) them with their main branch, often called 'master.'\n",
    "\n",
    "    b) What is the 'staging area', or 'index' in git? (1 pt)\n",
    "\n",
    "    The staging area lets developers add the changes they've made to \n",
    "    code before fully 'committing' the code, or adding it to the \n",
    "    tracked changes in git.\n",
    "\n",
    "    c) What does the command 'git commit' do? (1 pt)\n",
    "\n",
    "    \"git commit -m 'your commit message'\" will add the files in your \n",
    "    staging area or index to your tracked changes. It records changes \n",
    "    to the repository."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The bias of a coin is 0.6. What is the probability of flipping 8 or \n",
    "more heads in 10 flips?\n",
    "\n",
    "    ``` python \n",
    "    import scipy.stats as scs\n",
    "\n",
    "    rv = scs.binom(10, 0.6)\n",
    "\n",
    "    rv.pmf(8) + rv.pmf(9) + rv.pmf(10)\n",
    "\n",
    "    0.1672897536 \n",
    "    ```\n",
    "\n",
    "\n",
    "What is P(x=T | y=b)?\n",
    "\n",
    "  Since y=b, we need the relative probability that x=T within column \n",
    "  \"b\", i.e., \n",
    "  \n",
    "  (0.1) / (0.1 + 0.15) = 0.4\n",
    "  \n",
    "You have a continuous uniform distribution between 0 and 4. \n",
    "Calculate P(1 < x < 2 ∪ 3.5 < x < 4).\n",
    "\n",
    "P(1 < x < 2) = 1/4*(2-1)\n",
    "P(3.5 < x < 4) = 1/4(4-3.5)\n",
    "0.25 + 0.125 = 0.375\n",
    "\n",
    "Difference between PDF and PMF?\n",
    "Is the data continuous or discrete?   \n",
    "If the data is discrete, you have a PMF.   \n",
    "If the data is continuous, you have a PDF.\n",
    "\n",
    "You’re flipping an unfair coin 5 times and seeing how many times it comes up heads in those 5 flips.\n",
    "distribution.pmf(2) = 0.3394\n",
    "distribution.pdf(3): AttributeError\n",
    "distribution.cdf(5) = 1.0\n",
    "\n",
    "Which distribution is most appropriate to use to represent the scenario? \n",
    "\n",
    "The side that faces up when you roll a fair, 20 sided die: Uniform, Discrete\n",
    "In a fair die, each side is equally likely to face up. This is a uniform distribution. It is discrete because we can only have integer values (1,2,3, etc.).\n",
    "\n",
    "Whether or not a fair, 20 sided die will roll a 15 or greater: Bernoulli\n",
    "The Bernoulli distribution is used to describe a single trial that can have two outcomes: success or failure.\n",
    "\n",
    "You roll five of the 20 sided dice and want to know how many will roll a 15 or greater: Binomial\n",
    "The binomial distribution is used to predict the outcome of several Bernoulli (success/fail) trials.\n",
    "\n",
    "Busses arrive at a rate of 12 per hour. What is the probability 15 or more busses will arrive in the next hour? Poisson\n",
    "The Poisson distribution is used to predict number of events that occur.\n",
    "\n",
    "Busses arrive at a rate of 12 per hour. How long do you expect to wait until the next bus arrives? Exponential\n",
    "The exponential distribution is used to predict the wait time until the very first event. \n",
    "(Compare to the gamma distribution, which is used to predict the wait time until the k-th event).\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your employer sells disposable razors online. After analyzing a survey, \n",
    "the marketing team proposes giving a coupon for a free case of razors \n",
    "to every household that purchased razors from a competitor. Because \n",
    "customers must redeem coupons and make purchases using a registered \n",
    "account, your company will be able to determine if coupon recipients \n",
    "subsequently buy a case of razors. These customers will be considered \n",
    "'converted' for the purposes of business analyses.\n",
    "\n",
    "Your business development team within the company has identified three \n",
    "types of customers: Loyal, Fickle, and Indifferent. The CEO wants to \n",
    "build a stable revenue stream and prioritizes converting Loyal \n",
    "customers. The CEO asks you to analyze the marketing team's proposed \n",
    "campaign.\n",
    "\n",
    "- Your team determined that there are twice as many Loyal customers as \n",
    "Fickle customers, and that there are as many Indifferent customers as \n",
    "Fickle customers.\n",
    "- There is a 5% chance that a Loyal customer will be converted by \n",
    "receiving a coupon.\n",
    "- There is a 50% chance that a Fickle customer will be converted by \n",
    "receiving a coupon.\n",
    "- There is only a 20% chance that an Indifferent customer will be \n",
    "converted by a coupon.\n",
    "\n",
    "What's the probability that a converted customer is Loyal?\n",
    "\n",
    "    `F = {Fickle}, I = {Indifferent}, L = {Loyal}`\n",
    "\n",
    "    Calculate their probabilities.\n",
    "\n",
    "    `P(F) = P(I), P(L) = 2 * P(F), P(F) + P(I) + P(L) = 1`\n",
    "\n",
    "    `implies P(F) = P(I) = 0.25, P(L) = 0.50`\n",
    "\n",
    "    `C = {Converted}`\n",
    "\n",
    "    Find probability of converting a customer\n",
    "\n",
    "    `P(C|F) = 0.50, P(C|I) = 0.20, P(C|L) = 0.05.`\n",
    "\n",
    "    `P(C) = P(C|F) * P(F) + P(C|I) * P(I) + P(C|L) * P(L)`\n",
    "\n",
    "        = 0.50 * 0.25 + 0.20 * 0.25 + 0.05 * 0.50\n",
    "        \n",
    "        = 0.20\n",
    "\n",
    "    Use Bayes' theorem\n",
    "\n",
    "    `P(L|C) = P(C|L) * P(L) / P(C) `\n",
    "            `= (0.05 * .50) / 0.20 = 0.125`\n",
    "            \n",
    "\n",
    "Mongo\n",
    "\n",
    "    a) In terms of organization of the data in the database, how are \n",
    "    Mongo and SQL different? (2 pts)\n",
    "\n",
    "    SQL is relational, so it has a defined schema. The schema makes \n",
    "    sure that all the data in a column in a table has the same type - \n",
    "    all the dates are dates, all the numbers are either floats or ints, \n",
    "    etc. \n",
    "\n",
    "    Mongo is *not* relational, it's a NoSQL database where the \n",
    "    structure of the data is defined for each document, but not for the \n",
    "    table.\n",
    "\n",
    "\n",
    "    b) Give an example of data that is likely better handled by Mongo \n",
    "    than SQL. (1 pt)\n",
    "\n",
    "    Data whose structure changes rapidly is better handled in Mongo \n",
    "    than SQL. Web pages change frequently, so data obtained from \n",
    "    web-scraping is likely better handled by Mongo.\n",
    "\n",
    "\n",
    "What is Lazy Evaluation in Spark and why does it occur?\n",
    "\n",
    "    Lazy evaluation means that the execution will not begin until the \n",
    "    action is triggered. For Spark RDDs, there are two types of \n",
    "    operations: 1. Transformations and 2. Actions. In terms of Spark, \n",
    "    transformations do not occur until absolutely necessary (i.e, when \n",
    "    an action is called). Spark constructs a sequence of \n",
    "    transformations into a Directed Acyclic Graph (DAG). Once \n",
    "    completed, it is sent for execution to the cluster manager. Lazy \n",
    "    evaluation allows Spark to make optimization decisions with the \n",
    "    entire completed DAG. If each transformation was executed on \n",
    "    command, this would not be possible.\n",
    "\n",
    "\n",
    "What is an RDD in Spark?  How is a Spark Dataframe different?\n",
    "\n",
    "    A Resilient Distributed Dataset (RDD) is a fundamental data \n",
    "    structure in Spark. It is an immutable distributed collection of \n",
    "    objects. They can be created from HDFS, S3, HBase, JSON, text etc. \n",
    "    They are fault tolerant and can recover from errors (node failure, \n",
    "    slow processes) through the DAG (Directed Acyclic Graph). Spark \n",
    "    Dataframes, as they are built on top of RDDs, are also immutable \n",
    "    and lazily evaluated. Dissimilar to RDDs, spark dataframes have a \n",
    "    defined schema, which is just metadata about your data making your \n",
    "    data more structured, enabling performance enhancements. This also \n",
    "    allows you to use SQL-like syntax, unlike RDDs.\n",
    "\n",
    "\n",
    "What is Big Data?\n",
    "\n",
    "    - Data so large that it cannot be stored on one machine.\n",
    "    - Can Be...\n",
    "        * Structured: highly organized, searchable, fits into \n",
    "        relational tables\n",
    "        * Unstructured: no predefined format, multiple formats\n",
    "    - Often described as 3 Vs: (high volume, velocity, and variety)\n",
    "\n",
    "Predict website traffic woth 95% confidence based on sample\n",
    "\n",
    "    You could use the central limit theorem to determine the confidence \n",
    "    interval around the mean.  It would require using the sample's \n",
    "    mean, variance, and the number of samples drawn to calculate the \n",
    "    standard error of them mean, and that combined with the desired \n",
    "    significance level would allow you to calculate the confidence \n",
    "    interval.\n",
    "\n",
    "    Alternatively, you could use bootstrapping to create a confidence \n",
    "    interval.  Here you'd draw with replacement from the sample \n",
    "    creating many bootstrapped versions of the sample set.  For each \n",
    "    bootstrapped sample, calculate the mean.  Use the distribution of \n",
    "    the bootstrapped means to calculate the confidence interval (rank \n",
    "    from from low to high, throw out the bottom 2.5% and top 2.5%).\n",
    "    \n",
    "    Alternatively, you could use Bayesian methods to create a credible \n",
    "    interval. The posterior distribution of the data is proportional to \n",
    "    the likelihood of the data times the prior distribution of the \n",
    "    likelihood parameter(s). Here you might choose a Poisson \n",
    "    distribution for the likelihood and a gamma for the prior \n",
    "    distribution of the Poisson rate parameter, lambda. The resulting \n",
    "    posterior distribution can be used to form a probability interval \n",
    "    for the mean number of log-ins.\n",
    "\n",
    "### Not used in Feb 2021 cohort ####\n",
    "\n",
    "In Big O notation, what is the algorithmic complexity for the following \n",
    "functions (1pt each):\n",
    "\n",
    "    a)\n",
    "    def f(n):\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            j = 0\n",
    "            while j < n:\n",
    "                print(str(i) + \", \" + str(j))\n",
    "                j += 1\n",
    "            i += 1\n",
    "\n",
    "    O(n) = n^2\n",
    "\n",
    "    b)\n",
    "    def f(n):\n",
    "        i = 1\n",
    "        while i < n:\n",
    "            i *= 2\n",
    "            print(i)\n",
    "\n",
    "    O(n) = log(n)\n",
    "\n",
    "    To prove this, you can plot the number of loops it takes relative \n",
    "    to the size of n...\n",
    "    import matplotlib.pylab as plt\n",
    "        def f(n):\n",
    "            times = 0\n",
    "            i = 1\n",
    "            while i < n:\n",
    "                times += 1\n",
    "                i *= 2\n",
    "            return times\n",
    "        li = []\n",
    "        for i in range(1000):\n",
    "            li.append(f(i))\n",
    "        plt.plot(range(1000), li)\n",
    "        plt.plot(np.array(range(1000)), np.sqrt(np.array(range(1000))))\n",
    "        plt.plot(np.array(range(1000)), np.log2(np.array(range(1000))))\n",
    "        plt.legend(['true values', 'sqrt', 'log'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "    c)\n",
    "    def do_something_else(n):\n",
    "        print(\"My name is Inigo Montoya\")\n",
    "    O(n) = 1\n",
    "\n",
    "#######\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are building a model to predict price of homes in my neighborhood. My \n",
    "data set has 100 data points. We start by building two models using kNN with \n",
    "k = 1 and k = 99. Compare the two models in terms of the bias variance trade \n",
    "off. Which model is better?\n",
    "\n",
    "    A kNN model with k = 1 is a model with low bias and high variance. This model will fit\n",
    "    the training data very well but is highly dependent upon the data that happens to be in \n",
    "    this particular training set (high variance). A kNN model with k = 99 is a model with\n",
    "    high bias and low variance. You may recall from the kNN lecture that as we increased the\n",
    "    number of neighbors, the decision boundries became smoother. Jagged decision boundries is \n",
    "    a sign of high variance.\n",
    "\n",
    "    We cannot tell which model is better without comparing their performance on a validation set\n",
    "    or their performance across cross-validation sets.\n",
    "\n",
    "    A high bias model is very simple and ignores how the target varies as \n",
    "    features vary. A high bias model tends to underfit the data, leading to high \n",
    "    error on both training and testing sets.\n",
    "\n",
    "    A high variance model is very complex and fits all the patterns of variation \n",
    "    between the target and features (to the extent that the model is flexible \n",
    "    enough to capture all the variation.) A high variance model tends to overfit \n",
    "    the data, fitting noise/random variations in the data. This leads to low \n",
    "    error on a training set but very high error on a testing set.\n",
    "\n",
    "\n",
    "What are the assumptions behind OLS linear regression?\n",
    "\n",
    "    * Sample data is representative of the population\n",
    "    * True relationship between X and y is linear in the coefficients\n",
    "    * Feature matrix X is full rank \n",
    "    * Residuals are independent.\n",
    "    * Residuals are normally distributed.\n",
    "    * Variance of the residuals is constant (homoscedastic).\n",
    "\n",
    "\n",
    "You fit a linear regression to predict SAT score with many predictors, one \n",
    "of which is whether or not the student was homeschooled. `Beta_homeschooled = -40`.\n",
    "How do you interpret the coefficient? What might compromise the validity of the\n",
    "finding?\n",
    "\n",
    "    SHORT ANSWER:\n",
    "\n",
    "    If all other features remain unchanged, homeschoolers have a average SAT \n",
    "    score that is 40 points less than their non-homeschooled counterparts.  This\n",
    "    result can be tested against random chance using a statistical test under \n",
    "    the null hypothesis that the coefficient is 0.  The correctness of the \n",
    "    interpretation hinges on the truthfulness of (linear form of the) model; and\n",
    "    further on how realistic it is to change one feature while holding all other\n",
    "    features constant.  \n",
    "\n",
    "    If interpretation of coefficients is desired, the appropriateness of the model\n",
    "    (and it's assumptions) for the data at hand should be verified.  Prediction \n",
    "    from the model (as opposed to inference of the parameters in the model) is \n",
    "    much less sensitive model assumption violations.\n",
    "\n",
    "    LONGER ANSWER:\n",
    "\n",
    "    Since the sign on the coefficient is negative, being homeschooled is \n",
    "    associated with a _lower_ SAT score under this model. Under the model, it is\n",
    "    estimated that test takers that are homeschooled have SAT scores that are on\n",
    "    average _40 points lower_ than their non-homeschooled peers if _all else is equal_.\n",
    "\n",
    "    The ability to manipulate the model in this ``all else equal'' kind of way \n",
    "    assumes that we can vary the homeschooled characteristic independently without\n",
    "    changing the other variables.  This might not actually be a realistic \n",
    "    assumption if other characteristics affecting scores systematically differ \n",
    "    between homeschoolers and their non-homeschooled peers.  While the prediction\n",
    "    of the model to the data is unbiased, prediction of the model changing just \n",
    "    one variable and not the others may not be unbiased.  The model is fit on \n",
    "    the expectation of the covaration in the features.  It is designed to make \n",
    "    good predictions in response to this covariation.  By interpreting the effect \n",
    "    of changing one variable with all others held fixed you are forcing the model \n",
    "    away from how it is designed to fit the data.  \n",
    "\n",
    "    Coefficient interpretations are only true in the context of the model.  If \n",
    "    the linear form of the model is correct, or approximately so, then this \n",
    "    result provides an accurate (and potentially useful) characterization of the \n",
    "    real world.  If the model has not reasonably completely captured all the \n",
    "    variables at play in influencing test scores, then the results of the model \n",
    "    could be misleading and misattribute the roles and influences of the features\n",
    "    examined under the model. This goes back to the fact that there is covariance\n",
    "    structure in the features and they are dependent.  When one thing changes, \n",
    "    many things change.  The model utilizes this information when it is fit.  \n",
    "    But interpreting the coefficients is based on being to independently changes\n",
    "    features.\n",
    "\n",
    "    Hypothesis testing provides a way to examine if the association identified in\n",
    "    the model is a product of chance or justified as a true correlation observed\n",
    "    in the data.  Confidence intervals give us a way to estimate the plausible \n",
    "    range of the coefficients effect.  And testing the null hypothesis that the \n",
    "    coefficient is zero gives us a way to test if the association may not be \n",
    "    present (i.e., the coefficient is zero and the associated feature would thus \n",
    "    would not influence moel predictions).\n",
    "\n",
    "    Hypotheis testing relies on weather or not the model is a good fit for the data.\n",
    "    There are a number of assumptions at play here.  These requirements can be \n",
    "    examined with model diagnostics and remedial measures.  The assumption of the\n",
    "    linear form can also be diagnostically examined, but interpretation is still\n",
    "    dependent upon the \"all else held constant\" assumption, which is not necessarily\n",
    "    the state of the data as the model is fit (there is likely some dependency \n",
    "    between the features).* \n",
    "    \n",
    "\n",
    "You fit a logistic regression on the same data as above, this time to predict\n",
    "whether or not a student was admitted to a 4-year university. For the logistic \n",
    "model you get `Beta_homeschooled = -0.3`. How do you interpret this coefficient?\n",
    "Do we predict that more or less homeschoolers are admitted? Are 30% more/less \n",
    "homeschoolers admitted, something else? \n",
    "\n",
    "    SHORT ANSWER:\n",
    "\n",
    "    If all other features remain unchanged, since the sign on the coefficient is\n",
    "    negative, being homeschooled is associated with a _lower_ chance of admittance\n",
    "    under this model.  The interpretation of the coefficient is more complicated\n",
    "    than in linear regression as it is based on \"log-odds ratios\".  Hypothesis \n",
    "    testing of the coefficient can be used to examine if the observed association\n",
    "    may be a result of chance, and confidence intervals can give us an idea of the\n",
    "    plausible range of the coefficient value. \n",
    "\n",
    "    LONGER ANSWER;\n",
    "\n",
    "    Under the model, it is estimated that, all other features held constant, the\n",
    "    _odds_ of a homeschooled student being admitted are _lower_ than the odds of\n",
    "    a non-homeschooled student being admitted by a factor of `e^-0.3 = 0.74`.  \n",
    "    I.e., the _odds ratio (OR)_ of being of a homeschooled relative to being \n",
    "    non-homeschooled -- the odds of admittance for homeschooled students divided\n",
    "    by the odds of admittance for non-homeschooled students -- is `e^-0.3 = 0.74`\n",
    "    (with all other features held constant). Some additional notes are as follows.\n",
    "    (i) In logistic regression the predicted values are probabilities; however \n",
    "    (ii) _it is the LOG ODDS -- not the probabilities that are linear with respect\n",
    "    to the coefficients. (iii) In fact, the odds themselves are _multiplicative_\n",
    "    with respect to the coefficients, as indicated above (iv) `e^coef` is defined\n",
    "    to be the OR of a one unit increase in a covariate relative to the unchanged\n",
    "    covariate (_with all other covariates being equal_ and _assuming no \n",
    "    interactions in the model_).\n",
    "\n",
    "    Since we are given no information regarding a hypothesis test of this parameter,\n",
    "    e.g., a p-value, it is unlear in this case if this estimated association is \n",
    "    a result of chance or a robust association observed under this model or not.\n",
    "    To that end, and perhaps more informatively, it would be useful to examine a\n",
    "    confidence interval for the odds ratio of being admitted of homeschoolers \n",
    "    compared to non-homeschoolers.  This could give us a plausible range for the \n",
    "    effect of being homeschooled on admittance (as predicted by/under the model).\n",
    "    Such a confidence interval is a little tricky, however.  One must first create\n",
    "    a confidence interval for the coefficient, and then map the interval limits \n",
    "    through the link function into the odds ratio space to propegate the confidence\n",
    "    interval into that space.\n",
    "\n",
    "    It should also be noted that while model assumptions play less of a role in \n",
    "    logistic regression, all the above concerns and considerations (from problem\n",
    "    1) regarding interpretation of coefficients in linear regression have a role\n",
    "    here. I.e., the model is fit to predict outcomes given any dependencies in the\n",
    "    feature. Interpretation based on holding everyting constant and varying one \n",
    "    feature violates the way the model is fit.*\n",
    "\n",
    "\n",
    "Regularization is applied to a linear regression model and the \n",
    "following plots were generated.\n",
    "\n",
    "    a) The optimal value is roughly 120 (100 is acceptable) because this is the lowest point \n",
    "    on validation learning (test set) curve.\n",
    "\n",
    "    b) 3 coefficients are non-zero when lambda is set \n",
    "    to 150, so 3 features are used to make predictions.\n",
    "\n",
    "    c) Lasso imposes an L1 or absolute error penalty on the model \n",
    "    coefficients which has the effect of shrinking coefficients to zero \n",
    "    as regularization strength is increased. Ridge imposes an L2 or \n",
    "    squared error penalty which does not have the same property of \n",
    "    performing feature selection.\n",
    "\n",
    "\n",
    "Give an example of a confusion matrix with accuracy > 90%, but both precision\n",
    "< 10% and recall < 10%.\n",
    "\n",
    "    Accuracy is (TP+TN)/(TP+FP+TN+FN). \n",
    "\n",
    "             Predicted\n",
    "               +   -   \n",
    "             ---------\n",
    "          + | TP | FN |\n",
    "    Actual   ---------\n",
    "          - | FP | TN |\n",
    "             ---------\n",
    "\n",
    "\n",
    "    One possible solution:\n",
    "\n",
    "             Predicted\n",
    "               +   -\n",
    "             ---------\n",
    "          + | 01 | 91 |\n",
    "    Actual   ---------\n",
    "          - | 10 |9999|\n",
    "             ---------\n",
    "             \n",
    "\n",
    "We're building a model for a spam filter. We prefer to let spam messages go\n",
    "to the inbox rather than to let nonspam go to the spam folder. Interpreting a\n",
    "true positive as correctly identifying spam, which model should we choose (A or B)\n",
    "and why?\n",
    "\n",
    "    We do not want to send real mail to the spam folder.  Therefore, we want our\n",
    "    False Positive Rate (FPR) FP/(FP+TN) to be _low_.  Secondarily, having a \n",
    "    larger True Positive Rate (TRP) TP/(TP+FN) would be beneficial (as this is \n",
    "    supposed to be a spam filter, after all...).  Therefore, since the red curve\n",
    "    provides us with a higher TRP for low values of the FPR compared to the blue\n",
    "    curve, we prefer the red curve (Model B). \n",
    "\n",
    "    Note that when interpreting the ROC curve, FPR and TPR correspond _exactly_\n",
    "    to the notions of power/(1-beta) and specificity/(1-alpha), respectively. ROC\n",
    "    curves thus allows to view the alpha/beta tradeoff available for different \n",
    "    thresholds of our testing procedure and select a threshold that is most \n",
    "    suitable for our needs with respect to alpha and beta.\n",
    "    \n",
    "    \n",
    "Describe the process that takes place to compute a 5-fold cross-validation \n",
    "score. Name at least two ways that cross-validation is useful to a data \n",
    "scientist.\n",
    "\n",
    "    Partition the training set into 5 equal subsets. In turn, we will make each \n",
    "    subset the validation set and the remaining subsets the training set for our \n",
    "    model algorithm. The validation score is recorded for each turn/iteration. A \n",
    "    5-fold cross-validation will produce 5 validation scores. The scores are \n",
    "    averaged to produce the 5-fold cross-validation score for our model.\n",
    "\n",
    "    Cross-validation can be used to select the best model from a collection of \n",
    "    models. It can be used to select the optimal value of a hyperparameter. And \n",
    "    if performed properly, cross-validation can be used for feature selection."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Say you're building a Decision Tree Classifier on this dataset.\n",
    "\n",
    "    | color | number | label |\n",
    "    | ----- | ------ | ----- |\n",
    "    | blue  | 1      | 0     |\n",
    "    | blue  | 2      | 1     |\n",
    "    | red   | 1      | 0     |\n",
    "    | red   | 5      | 1     |\n",
    "\n",
    "    Splitting on what feature and value has the best information gain? Use your\n",
    "    intuition rather than calculating all the entropy values.\n",
    "\n",
    "    The objective here is to accurately predict the labels, thus we wish to make\n",
    "    a split which produces more uniform predictions than we already had before \n",
    "    we split the node.  Currently there are two 0's and two 1's, i.e., 50/50, in\n",
    "    the current node. Splitting on number for (<= 1) or (> 1) gives the best \n",
    "    information gain because the resulting leaves are pure in terms of the labels.\n",
    "    Splitting on color (red versus blue) does not improve upon the 50/50 prediction\n",
    "    we previously had.\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "You build a Decision Tree and get these stats:\n",
    "\n",
    "    train set accuracy:  90%\n",
    "    train set precision: 92%\n",
    "    train set recall:    87%\n",
    "\n",
    "    test set accuracy:   60%\n",
    "    test set precision:  65%\n",
    "    test set recall:     52%\n",
    "\n",
    "    What's going on? What would you do to modify the Decision Tree to fix the issue?\n",
    "\n",
    "    This decision tree is overfitting the data.  This is the case since it is \n",
    "    unable to produce the same levels of accuracy in the test set.  Very low and\n",
    "    mathcing percentages in the training and test set (that could be demonstratably\n",
    "    synchronously improved upon) would indicate underfitting.  Pruning this tree\n",
    "    in place would reduce overfitting.  Or we might impose some restrictions on\n",
    "    tree depth and node sizes and attempt tree construction again.\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Suppose instead you're building a Decision Tree Regressor. What is the \n",
    "information gain of this split of the data?\n",
    "\n",
    "    Parent Node: 6, 5, 8, 8, 5, 4, 2, 4, 4\n",
    "    Left Child Node: 6, 5, 8, 8\n",
    "    Right Child Node: 5, 4, 2, 4, 4\n",
    "\n",
    "    When doing classification, gini or entropy are used to measure decreasing \n",
    "    disorder and improved class prediction (i.e., information gain). However, \n",
    "    these are not applicable measures in continuous valued prediction (i.e., \n",
    "    regression) contexts and so prediction uncertainty reduction (i.e., information\n",
    "    gain) is instead measured on the basis of variance.  Specifically, information\n",
    "    gain in regression contexts is measured as the reduction in prediction variance\n",
    "    after splitting a node in a particular manner.  Pragmatically, when contructing\n",
    "    regression trees we can choose between alternative splitting options on the \n",
    "    basis of the reduction in prediction variance (i.e., information gain).  \n",
    "    E.g., the informaiton gain for the above split is given below we would prefer\n",
    "    this split over another if this gain is higher than for the other.\n",
    "\n",
    "    Var(Parent) = Var(6,5,8,8,5,4,2,4,4) = 3.4321\n",
    "    Var(Left Child) = Var(6,5,8,8) = 1.6875\n",
    "    Var(Right Child) = Var(5,4,2,4,4) = 0.9600\n",
    "\n",
    "    Gain = Var(S) - 4/9 * Var(A) - 5/9 * Var(B)\n",
    "         = 2.1488\n",
    "         "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "How are Random Forests different from standard Decision Trees?\n",
    "\n",
    "    (a) Random forests are built from an ensemble of decisions trees on the basis\n",
    "    of bootstrapped samples as opposed to standard (single) decision tree construction\n",
    "    which is based on the original sample. (b) Also unlike a standard decision \n",
    "    tree, each split in each tree of a random forest is made on the basis of a \n",
    "    randomly restricted set of features -- not all features may be used to create\n",
    "    a given split as is the case in a standard decision tree.  This random splitting\n",
    "    produces tree stuctures across the entire ensemble of trees that are somewhat\n",
    "    decorrelated, which in conjuction with averaging across a large number of \n",
    "    trees reduces the variance of the random forest ensemble.  (c) Because of \n",
    "    the power of this model variance reduction technique, unlike a standard decision\n",
    "    tree, random forest ensembles tend to use trees which are \"tall and bushy\" \n",
    "    (i.e., they only impose minimal restrictions on tree depth and node splitting)\n",
    "    and individually are very prone to overfitting.\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "In boosting, what is the relationship between the hyperparameters \n",
    "`learning_rate` and `n_estimators`?\n",
    "\n",
    "    It's an inverse relationship.\n",
    "    A decreased learning rate in boosting means each estimator has a decreased \n",
    "    impact on prediction.  The prediction improves (i.e.,  reduces its bias) \n",
    "    with each estimator, and so with a very slow learning rate it is likely that\n",
    "    more estimators will be required to achieve a given level of prediction \n",
    "    accuracy (i.e., a sufficient flexibility in the model to allow for a given \n",
    "    level of prediction accuracy).  Thus as `learning_rate` is made smaller \n",
    "    `n_estimators` will likely need to be made larger."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}