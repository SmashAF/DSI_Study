{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f589b74b5f9dfadb2b978c0f6d0c86213b22d6e1bb02d62665d77952f7aaa902"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.special import comb\n",
    "from scipy.stats import binom\n",
    "\n",
    "import operator\n",
    "from pylab import rcParams\n",
    "from itertools import islice\n",
    "\n",
    "from sklearn import neighbors, datasets, tree\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import log_loss, make_scorer, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer, load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands on ml2\n",
    "https://github.com/ageron/handson-ml2"
   ]
  },
  {
   "source": [
    "# Gradient Descent "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Gradient Descent is an optimizing algorithm used in Machine/ Deep Learning algorithms. The goal of Gradient Descent is to minimize the objective convex function f(x) using iteration.\n",
    "\n",
    "explain how gradient descent works\n",
    "minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.\n",
    "\n",
    "1) figure out which direction we can change the coefficients to make  \n",
    "L smaller. (how? partials.)    \n",
    "2) We adjust the coefficients slightly in the direction, (how? learning rate!)    \n",
    "3) recalculate the direction, re-adjust, and repeat, again and again until we converge.    \n",
    "\n",
    "limitations of gradient descent:         \n",
    "in is dependent on size of step. Too small takes a long time and too big might overshoot and will take many iterations to find minimum or may not find at all. \n",
    "     \n",
    "   \n",
    "challenges of gradient descent:        \n",
    "it requires all the data to be in memory at each step. That is a problem for Big Data situations when you have more data then can fit in memory.             \n",
    "only finds local extrema          \n",
    "static(what if you were getting data continuously?)      \n",
    "     \n",
    "\n",
    "use gradient descent to optimize the cost function for regression\n",
    "\n",
    "\n",
    "explain the advantage of stochastic gradient descent \n",
    "Stochastic Gradient Descent (SGD):       \n",
    "makes a step for each data point so only one is needed in memory at the same time.      \n",
    "with mini-batch stochastic gradient descent a small number of data points is used for each step    \n",
    "     \n",
    "SGD  properties:    \n",
    "1. allows inline training (can incorporate additional data)     \n",
    "2. Only requires one observation in memory at once.    \n",
    "3. the random data points help to prevent local minima.    \n",
    "4. faster than batch (regular) Gradient Descent on average.   \n",
    "5. prone to oscillation around optimum   \n",
    "\n",
    "    \n",
    "implement stochastic gradient descent (stochastic is how we actually do this)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Neural Networks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural network and deep learning will be used interchangeably\n",
    "\n",
    "neural networks perform well with high-dimensional (unstructured data)such as images, audio, and text\n",
    "\n",
    "Disadvantages:     \n",
    "hard to design and tune  \n",
    "slow to train   \n",
    "many local minima   \n",
    "uninterpretable   \n",
    "easy to overfit(needs lots of data)   \n",
    "   \n",
    "Advantages:    \n",
    "works well with high-dimensional data   \n",
    "can find almost anything, when designed correctly   \n",
    "online training   \n",
    "\n",
    "\n",
    "Explain the basic neural-network algorithms\n",
    "\n",
    "input layer       hidden layer      output layer\n",
    "x_1 O                   O\n",
    "x_2 O                   O                O - y \n",
    "x_3 O                   O \n",
    "x_4 O     \n",
    "\n",
    "   \n",
    "types of layers/neurons\n",
    " \n",
    "the nodes are neurons than can be input, hidden, or output. The input nodes are the features and once all the calculations are done, the value of the output nodes will be predicted labels. \n",
    "\n",
    "connections between the nodes can be weighta or activation functions. \n",
    "     \n",
    "Weights - the value for that neuron is the sum over all incoming connects of the weight times the value of the previous neuron.   \n",
    "     \n",
    "what are activation functions\n",
    "\n",
    "Activation Functions:\n",
    "Sigmoid \n",
    "Nice properties: range between 0 and 1 and we want to predict probability as output, differentiable, monotonic\n",
    "Drawback:\n",
    "get stuck during training\n",
    "\n",
    "Hyperbolic Tangent\n",
    "similar to sigmoid, range is -1 to 1\n",
    "\n",
    "ReLU\n",
    "probably most popular\n",
    "defined as the positive part of its argument:\n",
    "\n",
    "{\\displaystyle f(x)=x^{+}=\\max(0,x)}{\\displaystyle f(x)=x^{+}=\\max(0,x)}\n",
    "where x is the input to a neuron.\n",
    "Leaky ReLU\n",
    "Leaky ReLUs allow a small, positive gradient when the unit is not active.\n",
    "\n",
    "\n",
    "\n",
    "what are fully connected networks\n",
    "each layer is fully connected with each node is connected to each node in the previous layer.\n",
    "\n",
    "Explain the back propagation process\n",
    "done by using back propagation and is done by using backpropagated gradients using the chain rule and optimixed using an optimization algorithm by comparing the predicted outcome with the expected value for a set of inputs to find the output error, and propogate the error backwards, from one layer to the previous one based on the gradient of the intervening functions and weights.    \n",
    "\n",
    "Build a simple neural network in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Time Series"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "  - Define \"time series\" and \"time-series data\".\n",
    "a time series is a specific type of data where measurements of a single quantity are taken over time. \n",
    "\n",
    "ex. google trends\n",
    "\n",
    "\n",
    "  - Identify fundamental concepts in a time series: \n",
    "Various different time series often show common patterns. Attaching words to these patterns allows us to build a common language to discuss time series.\n",
    "\n",
    "trend - a gradual change in average level as time moves on and can be increasing, decreasing, or neither. \n",
    "\n",
    "seasonality - a pattern that tends to appear regularly and aligns with features of a calendar. \n",
    "\n",
    "stickiness -stickiness coefficient summarizes the extent to which deviations from the mean trajectory tend to co-vary over time. \n",
    "\n",
    "########################################################\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "detrended - remove a trend from a time series. When you detrend data you remove an aspect from the data that you think is causing some kind of distortion.\n",
    "\n",
    "deseasonalize - stripping data of its seasonal patterns is referred to as seasonally adjusted or deseasonalized data.\n",
    "\n",
    "moving average - creating a new series where the values are comprised of the average of raw observations in the original time series. \n",
    "\n",
    "window -   We essentially slide a window of a fixed side across our data, and average the values of the series within the window.\n",
    "\n",
    "The parameter w controls how far to the left and to the right of  y_i we look when averaging the nearby points, this is called the window.\n",
    "\n",
    "smaller values of window will tend to be influenced by noise of other non-trend paterns in a series, where large values produce smoother estimates of the general trend in the data. This makes large windows method prefered. \n",
    "\n",
    "\n",
    "remainder, error, residual -\n",
    "I_t, the irregular component (or \"noise\") at time t, which describes random, irregular influences. It represents the residuals or remainder of the time series after the other components have been removed.\n",
    "\n",
    "Statistical Concepts:\n",
    "  Random processes - Let's now impose a probability model on this data, i.e., now we consider series as random objects that can be sampled. When we need to distinguish the data from the statistical model that we are assuming generated the data, we call:\n",
    "\n",
    "The data are a series.\n",
    "The data generating process a random process, or more specifically, a time-series random process.\n",
    "\n",
    "  White noise -\n",
    "  The simplest possible random process from this perspective occurs when each Y i is independent from all the rest, and all the Y's are identically distributed with a mean of zero.\n",
    "In this case the series is called white noise.\n",
    "\n",
    "\n",
    "\n",
    "MA Series  \n",
    " - Use teh classical decomposition to decompose and then describe eatime series.\n",
    "\n",
    "\n",
    "  \n",
    "  - Define stationarity, contrast with independence.\n",
    "Independence\n",
    "Two events are statistically independent if the occurrence of one does not affect the probability of occurrence of the other. Let ùê¥ and ùêµ be two events then they are independent if and only if:\n",
    "ùëÉ(ùê¥‚à©ùêµ)=ùëÉ(ùê¥)ùëÉ(ùêµ)\n",
    "\n",
    "Stationarity\n",
    "A stationary process is a stochastic process whose unconditional joint probability distribution does not change when shifted in time.\n",
    "Let ùëãùë° be a stochastic process and ùêπùëã represent the cumulative distribution function of the unconditional joint distribution of ùëãùë°, then ùëãùë° is stationary if for all ùëò, ùúè and ùë°1,...,ùë°ùëò\n",
    "\n",
    "ùêπùëã(ùë•ùë°1+ùúè,...,ùë•ùë°ùëò+ùúè)=ùêπùëã(ùë•ùë°1,...,ùë•ùë°ùëò)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Identify a stationary time series.\n",
    "Time series are stationary if they do not have trend or seasonal effects. Summary statistics calculated on the time series are consistent over time, like the mean or the variance of the observations.\n",
    "\n",
    " -Random Walks\n",
    "In mathematics, a random walk is a mathematical object, known as a stochastic or random process, that describes a path that consists of a succession of random steps on some mathematical space such as the integers.\n",
    "\n",
    " The Augmented Dickey-Fuller test is setup as follows:\n",
    "\n",
    "H0: The series is not-stationary (specifically, has a unit root).\n",
    "Ha: The series is stationary.\n",
    "\n",
    "  - Fit ARIMA models to forecast a stationary time series.\n",
    "\n",
    "Autocorrelation\n",
    "\n",
    "partial autocorrelation\n",
    "\n",
    "autoregressive process\n",
    "\n",
    "\n",
    "ARMA models have both:\n",
    "\n",
    "A linear combination of white noise component (the MA), where subsequent values of the series are (partially) a linear combination of white noise.\n",
    "An autoregressive component the (AR), where subsequent values of the series are (partially) a linear combination of previous terms, plus noise.\n",
    "\n",
    "An ARIMA(p, d, q) model:\n",
    "Applies an ARMA model to a series that has been differenced d times.\n",
    "The AR part of the ARMA model has order p.\n",
    "The MA part of the ARMA model has order q.\n",
    "\n",
    "And, one final time, fitting an ARIMA model recovers the coefficients of a simulated ARIMA process."
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Decision Trees"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Can apply decision trees to classification/regression problems\n",
    "\n",
    "\n",
    "types of trees:  \n",
    "1. Classification Trees:\n",
    "our outcomes (target, output) are discrete. Leaf values are typically set to the most common outcomes.\n",
    "\n",
    "2. Regression Trees:\n",
    "our outcomes (target, output) are continuous. Leaf values are typically set to the mean value in oucomes. In Regression trees we use RSS instead of Gini/Entropy\n",
    "\n",
    "note: features (inputs, predictors) can be either discrete or continuous for both classification and regression trees.\n",
    "\n",
    "understand and can calcualte different measurements(Entropy, Gini) for quantifying branching in decision trees\n",
    "\n",
    "Gini impurity -the sum of(probability of object j in a set being identified correctly multiply by the probability it is identified incorrectly)\n",
    "\n",
    "Entropy - is nothing but the measure of disorder. (You can think of it as a measure of purity as well. You‚Äôll see. I like disorder because it sounds cooler.) Quantifies randomness\n",
    "\n",
    "apply methods to avoid overfitting\n",
    "Overfitting is likely if you build your tree all the way until every leaf is pure.\n",
    "    \n",
    "Prepruning ideas (prune while you build the tree):\n",
    "    ‚óèleaf size: stop splitting when num examples gets small enough\n",
    "    ‚óèdepth: stop splitting at a certain depth\n",
    "    ‚óèpurity: stop splitting if enough of the examples are the same class\n",
    "    ‚óègain threshold: stop splitting when the information gain becomes too small\n",
    "        \n",
    "Postpruning ideas (prune after you‚Äôve finished building the tree):\n",
    "    ‚óèmerge leaves if doing so decreases test-set error (very similar to how we removed features from our regression models using a function we called EliminateOne)\n",
    "\n",
    "\n",
    "Recursion\n",
    "Recursion  uses  the  idea  of  \"divide  and  conquer\"  to  solve  problems.  It  divides  a complex  problem  you  are  working  on  into  smaller  sub-problems  that  are  easily solved, rather than trying to solve the complex problem directly.Three Laws of Recursion1.A recursive algorithm must have a base case.2.A recursive algorithm must change its state and move toward the base case.3.A recursive algorithm must call itself, recursively.\n",
    "\n",
    "\n",
    "can apply recursive methods in your programming\n",
    "\n",
    "\n",
    "What are the pros and cons?\n",
    "   \n",
    "Pros    \n",
    "No feature scaling needed  \n",
    "Model nonlinear relationships - features can have    different effects at different nodes  \n",
    "Can do both classification and regression   \n",
    "Robust   \n",
    "Highly interpretable (unless we have a ton of features)\n",
    "Handle non-linear relationships well\n",
    "\n",
    "\n",
    "    \n",
    "Cons   \n",
    "Can be expensive to train - must consider (and discard) many possible splits for each node.   \n",
    "Often poor predictors because of high variance   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Random Forests"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    " Random  Forest Parameters:\n",
    "   Total number of trees\n",
    "   Number of features to use at each split\n",
    "   Individual decision tree Parameters\n",
    "         (tree depth, pruning, split criterion, e.t.c)\n",
    " \n",
    " \n",
    " \n",
    " * Explain the relationship and difference between bagging and a random forest.\n",
    "Bagging is when each tree of a random forest randomly samples, with replacement,  from the dataset, resulting in different trees. This allows improved variance without sacrificing bias. \n",
    "\n",
    "Bagging injects randomness in the selection of training data\n",
    "Random Forest uses randomness in both the training data and the features considered at each split\n",
    "More randomness  ‚ü∂ more decorrelation\n",
    "\n",
    "\n",
    " * Explain why bagging/random forests are more accurate than a single decision tree.\n",
    " Decision trees consider features individually so their response to various features changes more as data changes. \n",
    "Can have high variance. \n",
    "\n",
    "Rabdom forests have less correlation , making use of subspace sampling and bagging\n",
    "\n",
    "\n",
    "\n",
    " * Explain & construct a random forest (classification or regression).\n",
    "\n",
    "\n",
    "\n",
    " * Get feature importances from a random forest.\n",
    "\n",
    "\n",
    " \n",
    " * Explain how OOB error is calculated and what it estimates.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " ## Agenda\n",
    "\n",
    " * Discuss ensemble methods\n",
    "    combines many week models to form a strong model . multiple models are trained on the data. Each model is different and can be trained on different subsets, in different ways, or even different types of models.  \n",
    "       \n",
    " * Review bias/variance tradeoff\n",
    " * Review decision trees\n",
    " * Discuss bagging (bootstrap aggregation)\n",
    " * Discuss random forests\n",
    " * Discuss out-of-bag error\n",
    " * Discuss feature importance\n",
    "\n",
    "\n",
    "\n",
    " random features chosen at each split\n",
    " random forrest makes the column random and bagging makes the rows random\n",
    "\n",
    "\n",
    "\n",
    " Pros\n",
    "   Often give near state-of-the-art performance\n",
    "   Good out-of-the-box performance\n",
    "   No feature scaling needed\n",
    "   Model nonlinear relatipnships\n",
    "\n",
    "Cons\n",
    "   can be expensive to train (though can be done in parallel)\n",
    "   Not interpretable"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine learning algorythms are built to reproduce the data they were trained on. any bias within the training data will be evident within the algorythm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Gradient Boosted Regressors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost ,CatBoost, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}