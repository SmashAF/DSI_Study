{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Image Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Objectives:        \n",
    "1. Understand how images are represented in computers       \n",
    "          \n",
    "images are stored as a matrix of numbers that represent pixel intensity the larger the number the more saturated the pixel is. Color images are typically set up as three matrices setting the colors red, green, and blue and of three dimensions (height, width, and color). (all same size) Some may have a fourth, alpha channel, for transparency or store the colors in a different order thn the standard RGB.\n",
    "              \n",
    "            \n",
    "The first dimension is usually the height dimension starting from the top of the image.\n",
    "The second dimension is usually the width, starting from the left.\n",
    "The third dimension is usually the color. This is the \"channel\" dimension.\n",
    "For example, the 0th row (the top), the 0th column (the left), and the 0th color (usually red).\n",
    "      \n",
    "Attention:\n",
    "In numpy, and other libraries that rely on numpy, the order of axes is rows, columns, pages, this is different from what you might expect, if you are thinking in terms of x-position, y-position, z-position, because rows is the vertical or y-position and columns is the horizontal or x-position.\n",
    "      \n",
    "2. Issues of applying models to images             A) we have to unravel the image to make it flat losing the relationship of surrounding pixels.    \n",
    "    B) Lighting will affect the pixel values (a car will have a very different set of pixel values if it is cloudy vs sunny)\n",
    "    C)Humans are very good at  finding shapes and using those to classify an image. \n",
    "    D) you can think of shaoes/edges as the difference in adjoining pixels\n",
    "   \n",
    "3. Understand basic feature extraction      \n",
    "it may be useful to give the image edges, instead of pixel intensities, to train and predict on. there are multiple ways to do this: \n",
    "   \n",
    "use the gradient(rate of change) of the pixel intensities. look for the direction of 'color' change.(using grayscale from here on out)\n",
    "\n",
    "Sobel Operator - used to gain information about the pixels surrounding the single pixel. The convolution can be slid over the image to end up with a new image where evey pixel now represents numerically the surrounding pixels.   \n",
    "     \n",
    "The magnitude is calculated between these two edge detectors to get the final result.  \n",
    "\n",
    "normalization of images first is usually recommended.  \n",
    "       \n",
    "4. Show you a nice example of an image-based capstone, and make the case that figuring out and explaining **why** a machine learning model works can be **more interesting** and **more fun** than just optimizing it to get a good score.\n",
    "Know these when using neural networks          \n",
    "\n",
    "Four important tools (there are many more):\n",
    "1. Numpy\n",
    "2. scikit-image\n",
    "3. OpenCV\n",
    "4. PIL and Pillow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1.have to get output shape correct\n",
    "\n",
    "2.loss function that efectively trains across the shape\n",
    "\n",
    "3.activation function that matches the data for final activation\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If we have a color image how could we find out about aspects of that image?\n",
    "\n",
    "We can get the mean of each color in the image to get the mood\n",
    "More complex we can use K-means and look at centroids\n",
    "\n",
    "KMeans clustering is a common way to extract the dominant colors in an image.\n",
    "\n",
    "soft max \n",
    "lu\n",
    "\n",
    "\n",
    "binary cross entropy\n",
    "categorical cross entropy\n",
    "multi categorical cross entropy"
   ]
  },
  {
   "source": [
    "# Convolutional Neural Nets "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*critical to understand the shapes when working with neural networks\n",
    "\n",
    "sum of element-wise products - \n",
    "\n",
    "\n",
    "zero padding -add border of zeros around the original image to prevent filter form overhanging the ends\n",
    "\n",
    "striding - the number of rows/columns to move the filter. How it moves across the image. \n",
    "    stride = 1: \n",
    "    measure at every \n",
    "\n",
    "    stride > 1:\n",
    "\n",
    "    stride < 1:\n",
    "\n",
    "activation map:\n",
    "\n",
    "activation: amplifies significance where a filter aligns with image \n",
    "\n",
    "\n",
    "pooling (subsampling) layer - makes the representations smaller and more manageable, operates over each activation map independantly\n",
    "\n",
    "max pooling: pool with 2X2 layer and take the maximum value.\n",
    "\n",
    "loss functions -\n",
    "    regression\n",
    "        RMSE, MAE\n",
    "    classification\n",
    "        binary cross-entropy\n",
    "        multi-category cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image - filters - pooling - filters -pooling - flatten - predict outputs"
   ]
  },
  {
   "source": [
    "# NLP Natural Language Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for study - read lecture notebook as well\n",
    "Define NLP     \n",
    "Natural Language Processing is a subfield of machine learning focused on making sense of text. How to program computers to fruitfully process large amounts of natural language data.     \n",
    "\n",
    "Machine Learning (ML) -refers to systems that can learn from experience.     \n",
    "Language Processing (NLP) -refers to systems that can understand language.   \n",
    "\n",
    "Sparsity: \n",
    "    very large number of words in any language\n",
    "    relatively small number of words in any single document\n",
    "    large precentage of stop words\n",
    "    two similar documents might contain very few shared words other than stop words\n",
    "    an X matrix of words would be almost entirely NA\n",
    "Ambiguity: \"I made her duck\"\n",
    "    I cooked dinner  of waterfowl for her\n",
    "    I cooked her woaterfowl\n",
    "    I created the duck she owns\n",
    "    I caused her to quickly lower her head as if to avoid a thrown object\n",
    "    I magically turned her into a duck\n",
    "    Formerly known as word sense ambiguity\n",
    "        \n",
    "Social Factors of Language\n",
    "    differences in class/status\n",
    "    often formal datasets used to train on (New York Times)\n",
    "    regional dialects and slang\n",
    "    may lead to discounted value statements from people with different dialects\n",
    "    politeness\n",
    "    style of speech depends on difference in age/gender/social status of person speaking and being addressed\n",
    "         \n",
    "Confused Meaning\n",
    "\"Well, I'm not going to lie, it's not like I wasn't going to tell him that I didn't like the resaurant he chose\"\n",
    "   \n",
    "Intentional Hidden Meaning\n",
    "\"Well, at least the cookies were gluten free\"\n",
    "   \n",
    "\n",
    "Branches of NLP\n",
    "Phonetics and Phonology - linguistic sounds\n",
    "Morphology and Semantics - meanings of words and components of words\n",
    "Pragmatics - meaning with respect to goals and intentions  \n",
    "Discourse - structure of language larger that a single utterance\n",
    "Cryptology - dtermining patterns and meaning from npon-standard input\n",
    "\n",
    "Zipf's  Law\n",
    "an empirical law formulated using mathematical statistics that refers to the fact that for many types of data studied in the physical and social sciences, the rank-frequency distribution is an inverse relation. \n",
    "Aside: Voynich Manuscript\n",
    "\n",
    "Vocabulary:\n",
    "Document -     \n",
    "* a single email, product review, customer request/complaint, incident report   \n",
    "* usually assumed to have a single author, single topic, single intent   \n",
    "* corresponds to a single row in an X input matrix (an X ventor)    \n",
    "     \n",
    "Corpus -     \n",
    "* a collection of documents   \n",
    "* from multiple authors, topics, subjects and intents\n",
    "* corresponds to the X matrix\n",
    "      \n",
    "Stop-Words -    \n",
    "* common or domain-specific words    \n",
    "* not useful in differentiating documents   \n",
    "* generally removed   \n",
    "\n",
    "Tokens -   \n",
    "* the components of a document (words, n-grams, phrases, fragments)    \n",
    "* stemmed {car, cars, car's, cars'} -> car\n",
    "* lemmatized {bring, brought, bringing, brung} -> bring   \n",
    "   \n",
    "N-grams -    \n",
    "* more than one word that commonly appear together and may have a meaning distinct from component words: 'Star Wars', 'New York Times'      \n",
    "            \n",
    "Bag-of-words: Lots of simplifying assumptions   \n",
    "* word (token) count is interpreted as importance    \n",
    "* word order and association are ignored  \n",
    "\n",
    "Name and describe the steps necessary for processing text in machine learning.\n",
    "1. lowercase all text\n",
    "2. strip out punctuationand miscellaneous spacing\n",
    "3. remove stop words\n",
    "4. stem or lemmatize into tokens (decrease sparsity, increase density)\n",
    "5. convert to sparse numeric representation\n",
    "    * counts\n",
    "    * term frequency (tf)\n",
    "    * term frequency-inverse document frequency (tf-idf)\n",
    "6. train/cluster machine learning model\n",
    "7. optional: part-of-speech tagging, expand feature matrix with n-grams\n",
    "\n",
    "\n",
    "Information retrieval. How do you find a document or a particular fact within a document?\n",
    "Document classification. What is the document about amongst mutually exclusive categories?\n",
    "Machine translation. How do you write an English phrase in Chinese? Think of Google translate.\n",
    "Sentiment analysis. Was a product review positive or negative? Natural language processing is a huge field and we will just touch on some of the concepts.\n",
    "- Implement a Natural Language Processing pipeline.\n",
    "- Explain the cosine similarity measure and why it is used in NLP.\n",
    "\n",
    "\n",
    "Describe several use cases   \n",
    "conversational agents (Siri, Cortana, Google Home, Alexa)    \n",
    "Machine Translation (Google translation, handheld devices)   \n",
    "Speech Recognition (legal and medical documentation)   \n",
    "Sentiment Analysis   \n",
    "Historical Research   \n",
    "\n",
    "Difficulties/Opportunities   \n",
    "    problems with Bag-of-Words: dstruggles with short, concise documents ('I really do like this product') nearly identical to ('I really do not like this product') very different than ('this device is great')\n",
    "\n",
    "Loss of word Order/Association\n",
    "    this device is bad. it did not solve my problem\n",
    "    this device is bad. it did solve my problem. \n",
    "    Solution: use relatively large documents \n",
    "\n",
    "Inexact Correlation Between Importance and Expression\n",
    "    subtly: important words are not used frequently\n",
    "    verbosity: lots of words are not used, thereby making all words seem important\n",
    "    succinctness: fe words are used, making their count seem less important\n",
    "\n",
    "$Introduce advanced method$ \n",
    "*Sentiment Analysis:\n",
    "words have been manually placed into categories\n",
    "large overlap between categories\n",
    "accuracy between categories\n",
    "accuracy subject to interpretation\n",
    "\n",
    "*Latent Dirichlet Allocation:\n",
    "documents belong to groups that are characterized by topic, rather tham specific words\n",
    "topics are not specified, but deduced from Bayesian probability of term co-occurence\n",
    "Documents are considered by their probability of having been generated from a  mixture of topics\n",
    "therefore, two documents may be measured similar even is they share relatively few words. \n",
    "\n",
    "*LongShortTermMemory (LSTM): \n",
    "A type of neural  network that takes input both from current data and a memory element of the network\n",
    "the network state retains information from previous iterations until they are explicitly reset by new information\n",
    "useful for connecting pronouns to meaning(among other things)\n",
    "\n",
    "*Word Embedding:\n",
    "difference between 'word space' and 'meaning space'\n",
    "in tf-idf, the word king is just a numenric value in a single column, alternatively, king can be a location on a multi dimensional system:\n",
    "gender axis - King -> Queen\n",
    "parental axis - King -> Prince\n",
    "authority axis - King -> Subject\n",
    "\n",
    "*Word2Vec\n",
    "deep neural networks to convert from word vectors to meaning vectors(with numeric values)\n",
    "allows for vector representation in math\n",
    "\n",
    "*BERT and ELMo\n",
    "    %ELMo: Similar to Word2Vec, but accounts for homonyms ('river bank' vs. 'bank account')\n",
    "    %Bert: \"Bidirectional Encoder Representations from Transformers\"\n",
    "    similar to ELMo, but accounts for context ('running a race' vs 'running a company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use .fit_transform for training data and .fit for test data"
   ]
  },
  {
   "source": [
    "### sklearn "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alculate tf and tf-idf matrices   \n",
    "tf-idf in sklearn \n",
    "\n",
    "1) sklearn.feature _extraction.text.CountVectorizer\n",
    "    + lots of built in operations like lowercase, tokenizer, stop_words, and N-grams\n",
    "    + lots of options like min and max document frequencies, domain-specific vocabulary, and max features\n",
    "    + output options(sparse matrix) Binary, Token count, Vocabulary_\n",
    "\n",
    "2) sklearn.feature_extraction.text.TfidfVectorizer\n",
    "    + lots of built in operations like lowercase, tokenizer, stop_words, and N-grams\n",
    "    + lots of options like min and max document frequencies, domain-specific vocabulary, and max features\n",
    "    + output options(sparse matrix) Binary, TTF-IDF, Vocabulary_\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "# NAIVE Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes rule - probability of an event, based on prior knowledge of conditions that might be related to the event. \n",
    "\n",
    "marginal probability - observing A and B independently of each other. \n",
    "\n",
    "conditional probability - likelihood of event A occuring given B is true.\n",
    "\n",
    "important: due to the bag of words featurization of text, where all features are assumed independent (naive), the input feture matrix is often very wide (~10000, 50000) and can be even greater that the number of data samples. Naive Bayes computationally efficient (is just sums). \n",
    "\n",
    "uses: classifying emails (spam or not), classifying news articles into genres, sentiment analysis (pos or neg review)\n",
    "\n",
    "Naive Bayes Classifier - calculate the probability that a document belongs to each class and choose the class with the highest probability. To do this calculate:\n",
    "*Priors: the probability that a generic document belongs to each class \n",
    "\n",
    "P(y_c) = num of articles in class c / total num of articles\n",
    "ex 8 articles 3 Sports, 4 politics, 1 arts\n",
    "sports = 3/8 = 0.375\n",
    "politics = 4/8 = 0.5\n",
    "arts = 1/8 = 0,125\n",
    "*Conditional Probabilities(Likelihoods): the probability that each wors appears in each class. \n",
    "\n",
    "P(w_i | y_c) = num pf times w_i appears in articles of class y_c / total num of words in articles of class y_c\n",
    "\n",
    "ex P(sports) * 'ball' / word count of sports articles\n",
    "\n",
    "maximum likelihood estimation: \n",
    "\n",
    "laplace smoothing = the word has never appeared before in a document of the class. The prob would be 0. in this situation we add 1 to the numerator and number of words in the vocabulary  to denominator. \n",
    "\n",
    "preventing numerical underflow : \n",
    "take the log of both sides. \n",
    "\n",
    "ways to compare documents - \n",
    "    Euclidean distance\n",
    "\n",
    "\n",
    "\n",
    "Naive Bayes Pros and Cons:\n",
    "\n",
    "*Pros\n",
    "    good with wide data (p >> n)\n",
    "    good if n is small or n is quite big\n",
    "    fast to train (it's just counting)\n",
    "    good at online learning, streaming data, learns by processing one datapoint at a time\n",
    "    simple to implement, not necessarily memory-bound (DB implementations)\n",
    "    multi-class classification\n",
    "*Cons\n",
    "    naive assumption means correlated features are not actually trated right\n",
    "    sometimes outperformed bu other models\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Clustering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* What is the difference between \n",
    "**supervised** \n",
    "try to predict labels and target values. \n",
    "    linear and logistic regression with lasso or ridge regularization\n",
    "    decision trees, bagging, random forest, boosting\n",
    "    KNN\n",
    "Label == target == endogenous variable == dependent variable == y\n",
    "\n",
    "**unsupervised** learning\n",
    "no labels. no target\n",
    "    reasons for use:\n",
    "    EDA\n",
    "    Discovering latent variables\n",
    "    feature engineering \n",
    "    preprocessing\n",
    "    autoencoding - for making smaller approximations/representations of the original data as in data compression\n",
    "\n",
    "Clustering:\n",
    "●Data Exists\n",
    "●Distributed Across Multiple Dimensions\n",
    "●Reason to believe (or suspect) that difference exist in the data that have not been explicitly labeled\n",
    "●Reason to believe (or suspect) that knowing these differences helps make decisions, or improve outcomes\n",
    "\n",
    "a cluster is good when the points are near to each other and far from points outside the cluster. \n",
    "minimize \"within cluster variance\"\n",
    "\n",
    "clustering great for EDA, data splitting, nd feature generation. \n",
    "\n",
    "\n",
    "goal is to divide the data into distinct subgroups\n",
    "\n",
    "K-means ivolves repeatedly assigning points to clusters and then gfinding new clusters based on those points. \n",
    "\n",
    "* What are the steps of the **k-means** clustering algorithm?\n",
    "    choose a number of clusters k\n",
    "    randomly assign each point to a cluster\n",
    "    repeat:\n",
    "        a. for each of k clusters, compute cluster centroid by taking mean vector of points in cluster.\n",
    "        b. assign each data point to cluster for which centroid is closest (Euclidean 'L2')\n",
    "\n",
    "repeat until one of these is met:\n",
    " 1) specified number of iterations.\n",
    " 2) centroids do not change at all/no change to clusters\n",
    " 3) centroids don't move by much\n",
    " \n",
    "note: kmeans calculates a local median which is sometimes a bad one. An alternative would be to choose centroids based on random points to keep centroids farther apart. \n",
    "\n",
    "kmeans ++ \n",
    "    Choose one point for first center.\n",
    "    Repeat:\n",
    "        Calculate distance from each point to the nearest center $d_i$\n",
    "        Choose a point to be the next center, randomly, using a weighed probability $d_i^2$\n",
    " ... until k centers have been chosen.\n",
    "\n",
    "\n",
    "Non-Deterministic\n",
    "●Due to random initialization, it is not assured that points will always be assigned to the same cluster\n",
    "●Especially if clusters are not well separated or wrong value of k is chosen\n",
    "●May be necessary to repeat clustering multiple times\n",
    "\n",
    "* How does one choose the best k? \n",
    "there are several methods. \n",
    "    The Elbow Method\n",
    "    Silhouette Score\n",
    "    GAP Statistics\n",
    "(within-cluster sum of squares mentioned. problem is more clusters ==> lower WCSS )\n",
    "\n",
    "* What is the **elbow method**? \n",
    "  plot within-cluster sum of squares and try to see what looks like an elbow. Sometimes there may not be a clear elbow to choose from. \n",
    "\n",
    "**silhouette score**?\n",
    "    for each point x_i:\n",
    "        a_i is the mean distance between x_i and all other points in the same cluster. \n",
    "        b_i is the mean distance between x_i and all points in the nearest cluster.\n",
    "\n",
    "        silhouette(x_i) = (b_i - a_i) / max(a_i,b_i)\n",
    "\n",
    "    the silhouette score is the average of silhouette score of all points. the higher the score, the tighter and more separated the clusters.\n",
    "\n",
    "possible scores:\n",
    "    near 1: very small tight cluster\n",
    "    0 : at the edge of two clusters, could be in either\n",
    "    < 0: oops.\n",
    "\n",
    "Assumptions\n",
    "●Correct k \n",
    "●Equal Variance\n",
    "●Isotropic Shape\n",
    "●Clusters do not need same number of points\n",
    "\n",
    "* How does the **curse of dimensionality** affect clustering?\n",
    "random variation in extra dimensions can hide significant differences between clusters. the more dimensions there are, the worse the problem.\n",
    "\n",
    "hierachical clustering:\n",
    "    1)assign each point to its own cluster\n",
    "    2)Repeat:\n",
    "        compute distences between clusters\n",
    "        merge closest clusters\n",
    "     ...until all are merged\n",
    "\n",
    "\n",
    "* What is the difference between k-means and **hierarchical clustering**?\n",
    "in hierarchical clustering the we do not declare a number of clusters. They are built agglomeratively, and we can choose whatever level wanted afterward.\n",
    "\n",
    "Defining distance between clusters: Linkage\n",
    "    - between clusters, the difference is less clear then with points. There are a few methods we can use. \n",
    "    1) Single: minimum pairwisedissimilarity between points in cluters(not as good and can lead to long narrow clusters)\n",
    "    2) Complete: maximum pairwise dissimilarity between points in clusters (good)\n",
    "    3) Average: average of pairwise dissimilarity between points in clusters (also good)\n",
    "    4) Ward/Centroid: dissimilarity between centroids (used in genomics; risk of inversions)\n",
    "\n",
    "DBSCAN (Density-Based Spacial Clustering of Applications with Noise) we don't specify the number of clusters. Instead we specify:\n",
    "    1) epsilon: distance between points for them to be connected\n",
    "    2) minPts: number of conected points for a point to be a 'core' pooint\n",
    "\n",
    "A cluster is all connected core points, plus others within epsilon of one of those. Other points are noise.\n",
    "\n",
    "\n",
    "Although simple generic prescriptions for choosing the individual attribute dissimilarities ... can be comforting, there is no substitute for careful thought in the context of each individual problem. Specifying an appropriate dissimilarity measure is far more important in obtaining success with clustering than the choice of clustering algorithm"
   ]
  },
  {
   "source": [
    "### Optional Content: Distribution-based clustering\n",
    "\n",
    "With distribution-based clustering we assume some fixed number of clusters, and assume they follow some (often normal) distribution. We then try to find the parameters that have the **maximum likelihood** of producing these data.\n",
    "\n",
    "This is more difficult then other problems we've seen because we don't know which point came from which distribution. We need to add some hidden variables to the problem: the probability each point came from each distribution. We can solve this by an **expectation-maximization** (EM) algorithm in which we alternate between expectation steps (where we calculate the hidden variables) and maximization steps (in which we calculate the maximum-likelihood parameters assuming the hidden variables are correct)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Principal-Component Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# SVD Singular Value Decomposition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem - large data - matrix of photos with pixels 500x500\n",
    "\n",
    "SVD - because \n",
    "\n",
    "\n",
    "principal component\n",
    "\n",
    "latent features (latent topic)\n",
    "\n",
    "latent vs explicit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Topic Modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}