{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f589b74b5f9dfadb2b978c0f6d0c86213b22d6e1bb02d62665d77952f7aaa902"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices indexing pandas\n",
    "train test split sklearn\n",
    "hyperparameters\n",
    "sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Image Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Objectives:        \n",
    "1. Understand how images are represented in computers       \n",
    "          \n",
    "images are stored as a matrix of numbers that represent pixel intensity the larger the number the more saturated the pixel is. Color images are typically set up as three matrices setting the colors red, green, and blue and of three dimensions (height, width, and color). (all same size) Some may have a fourth, alpha channel, for transparency or store the colors in a different order thn the standard RGB.\n",
    "              \n",
    "            \n",
    "The first dimension is usually the height dimension starting from the top of the image.\n",
    "The second dimension is usually the width, starting from the left.\n",
    "The third dimension is usually the color. This is the \"channel\" dimension.\n",
    "For example, the 0th row (the top), the 0th column (the left), and the 0th color (usually red).\n",
    "\n",
    "\n",
    "Let's review the data types of numbers:\n",
    "\n",
    "uint8 is an unsigned, 8 bit integer. 2^8 = 256, so uint8 values go from 0 - 255. In a grayscale image, 0 will be black and 255 white. This is the most common format.\n",
    "uint16 also unsigned, 16 bit. 2^16 = 65536. Twice the memory of a uint8 value.\n",
    "float a float value (usually 64 bit) between 0-1. 0 black, 1 white.\n",
    "\n",
    "\n",
    "Attention:\n",
    "In numpy, and other libraries that rely on numpy, the order of axes is rows, columns, pages, this is different from what you might expect, if you are thinking in terms of x-position, y-position, z-position, because rows is the vertical or y-position and columns is the horizontal or x-position.\n",
    "      \n",
    "2. Issues of applying models to images:             \n",
    "    A) we have to unravel the image to make it flat losing the relationship of surrounding pixels.    \n",
    "    B) Lighting will affect the pixel values (a car will have a very different set of pixel values if it is cloudy vs sunny)\n",
    "    C)Humans are very good at  finding shapes and using those to classify an image. \n",
    "    D) you can think of shapes/edges as the difference in adjoining pixels\n",
    "   \n",
    "3. Understand basic feature extraction      \n",
    "it may be useful to give the image edges, instead of pixel intensities, to train and predict on. there are multiple ways to do this: \n",
    "   \n",
    "use the gradient(rate of change) of the pixel intensities. look for the direction of 'color' change.(using grayscale from here on out)\n",
    "\n",
    "Sobel Operator - used to gain information about the pixels surrounding the single pixel. The convolution can be slid over the image to end up with a new image where evey pixel now represents numerically the surrounding pixels.   \n",
    "     \n",
    "The magnitude is calculated between these two edge detectors to get the final result.  \n",
    "\n",
    "normalization of images first is usually recommended.  \n",
    "\n",
    " if you do not care about color make it grayscale. \n",
    " \n",
    "       \n",
    "4. Show you a nice example of an image-based capstone, and make the case that figuring out and explaining **why** a machine learning model works can be **more interesting** and **more fun** than just optimizing it to get a good score.\n",
    "Know these when using neural networks          \n",
    "\n",
    "Four important tools (there are many more):\n",
    "1. Numpy\n",
    "2. scikit-image\n",
    "3. OpenCV\n",
    "4. PIL and Pillow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1.have to get output shape correct\n",
    "\n",
    "2.loss function that efectively trains across the shape\n",
    "\n",
    "3.activation function that matches the data for final activation\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If we have a color image how could we find out about aspects of that image?\n",
    "\n",
    "We can get the mean of each color in the image to get the mood\n",
    "More complex we can use K-means and look at centroids\n",
    "\n",
    "KMeans clustering is a common way to extract the dominant colors in an image.\n",
    "\n",
    "soft max \n",
    "lu\n",
    "\n",
    "\n",
    "binary cross entropy\n",
    "categorical cross entropy\n",
    "multi categorical cross entropy"
   ]
  },
  {
   "source": [
    "# Convolutional Neural Nets "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Convolutional neural networks do 3 things to make image calssification practical: \n",
    "    1) reduce number of output nodes\n",
    "    2) tolerate small shifts in where pixels are in the image\n",
    "    3) take advantage of the correlations that we observe in complex images\n",
    "\n",
    "\n",
    "*critical to understand the shapes when working with neural networks\n",
    "\n",
    "sum of element-wise products - \n",
    "\n",
    "\n",
    "zero padding -add border of zeros around the original image to prevent filter form overhanging the ends\n",
    "\n",
    "striding - the number of rows/columns to move the filter. How it moves across the image. \n",
    "    stride = 1: \n",
    "    measure at every \n",
    "\n",
    "    stride > 1:\n",
    "\n",
    "    stride < 1:\n",
    "\n",
    "activation map:\n",
    "\n",
    "activation: amplifies significance where a filter aligns with image \n",
    "\n",
    "\n",
    "pooling (subsampling) layer - makes the representations smaller and more manageable, operates over each activation map independantly\n",
    "\n",
    "max pooling: pool with 2X2 layer and take the maximum value.\n",
    "\n",
    "loss functions -\n",
    "    regression\n",
    "        RMSE, MAE\n",
    "    classification\n",
    "        binary cross-entropy\n",
    "        multi-category cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image - run through convolution filters - feature map - filter (ReLU activation function) - max pooling - flatten input into neural network - predict outputs"
   ]
  },
  {
   "source": [
    "# NLP Natural Language Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document and term frequency\n",
    "k means\n",
    "picking best algorythom\n",
    "\n",
    "for study - read lecture notebook as well\n",
    "Define NLP     \n",
    "Natural Language Processing is a subfield of machine learning focused on making sense of text. How to program computers to fruitfully process large amounts of natural language data.     \n",
    "\n",
    "Machine Learning (ML) -refers to systems that can learn from experience.     \n",
    "Language Processing (NLP) -refers to systems that can understand language.   \n",
    "\n",
    "Sparsity: \n",
    "    very large number of words in any language\n",
    "    relatively small number of words in any single document\n",
    "    large precentage of stop words\n",
    "    two similar documents might contain very few shared words other than stop words\n",
    "    an X matrix of words would be almost entirely NA\n",
    "Ambiguity: \"I made her duck\"\n",
    "    I cooked dinner  of waterfowl for her\n",
    "    I cooked her woaterfowl\n",
    "    I created the duck she owns\n",
    "    I caused her to quickly lower her head as if to avoid a thrown object\n",
    "    I magically turned her into a duck\n",
    "    Formerly known as word sense ambiguity\n",
    "        \n",
    "Social Factors of Language\n",
    "    differences in class/status\n",
    "    often formal datasets used to train on (New York Times)\n",
    "    regional dialects and slang\n",
    "    may lead to discounted value statements from people with different dialects\n",
    "    politeness\n",
    "    style of speech depends on difference in age/gender/social status of person speaking and being addressed\n",
    "         \n",
    "Confused Meaning\n",
    "\"Well, I'm not going to lie, it's not like I wasn't going to tell him that I didn't like the resaurant he chose\"\n",
    "   \n",
    "Intentional Hidden Meaning\n",
    "\"Well, at least the cookies were gluten free\"\n",
    "   \n",
    "\n",
    "Branches of NLP\n",
    "Phonetics and Phonology - linguistic sounds\n",
    "Morphology and Semantics - meanings of words and components of words\n",
    "Pragmatics - meaning with respect to goals and intentions  \n",
    "Discourse - structure of language larger that a single utterance\n",
    "Cryptology - dtermining patterns and meaning from npon-standard input\n",
    "\n",
    "Zipf's  Law\n",
    "an empirical law formulated using mathematical statistics that refers to the fact that for many types of data studied in the physical and social sciences, the rank-frequency distribution is an inverse relation. \n",
    "Aside: Voynich Manuscript\n",
    "\n",
    "Vocabulary:\n",
    "Document -     \n",
    "* a single email, product review, customer request/complaint, incident report   \n",
    "* usually assumed to have a single author, single topic, single intent   \n",
    "* corresponds to a single row in an X input matrix (an X ventor)    \n",
    "     \n",
    "Corpus -     \n",
    "* a collection of documents   \n",
    "* from multiple authors, topics, subjects and intents\n",
    "* corresponds to the X matrix\n",
    "      \n",
    "Stop-Words -    \n",
    "* common or domain-specific words    \n",
    "* not useful in differentiating documents   \n",
    "* generally removed   \n",
    "\n",
    "Tokens -   \n",
    "* the components of a document (words, n-grams, phrases, fragments)    \n",
    "* stemmed {car, cars, car's, cars'} -> car\n",
    "* lemmatized {bring, brought, bringing, brung} -> bring   \n",
    "   \n",
    "N-grams -    \n",
    "* more than one word that commonly appear together and may have a meaning distinct from component words: 'Star Wars', 'New York Times'      \n",
    "            \n",
    "Bag-of-words: Lots of simplifying assumptions   \n",
    "* word (token) count is interpreted as importance    \n",
    "* word order and association are ignored  \n",
    "\n",
    "Name and describe the steps necessary for processing text in machine learning.\n",
    "1. lowercase all text\n",
    "2. strip out punctuationand miscellaneous spacing\n",
    "3. remove stop words\n",
    "4. stem or lemmatize into tokens (decrease sparsity, increase density)\n",
    "5. convert to sparse numeric representation\n",
    "    * counts\n",
    "    * term frequency (tf)\n",
    "    * term frequency-inverse document frequency (tf-idf)\n",
    "6. train/cluster machine learning model\n",
    "7. optional: part-of-speech tagging, expand feature matrix with n-grams\n",
    "\n",
    "\n",
    "Information retrieval. How do you find a document or a particular fact within a document?\n",
    "Document classification. What is the document about amongst mutually exclusive categories?\n",
    "Machine translation. How do you write an English phrase in Chinese? Think of Google translate.\n",
    "Sentiment analysis. Was a product review positive or negative? Natural language processing is a huge field and we will just touch on some of the concepts.\n",
    "- Implement a Natural Language Processing pipeline.\n",
    "- Explain the cosine similarity measure and why it is used in NLP.\n",
    "\n",
    "\n",
    "Describe several use cases   \n",
    "conversational agents (Siri, Cortana, Google Home, Alexa)    \n",
    "Machine Translation (Google translation, handheld devices)   \n",
    "Speech Recognition (legal and medical documentation)   \n",
    "Sentiment Analysis   \n",
    "Historical Research   \n",
    "\n",
    "Difficulties/Opportunities   \n",
    "    problems with Bag-of-Words: dstruggles with short, concise documents ('I really do like this product') nearly identical to ('I really do not like this product') very different than ('this device is great')\n",
    "\n",
    "Loss of word Order/Association\n",
    "    this device is bad. it did not solve my problem\n",
    "    this device is bad. it did solve my problem. \n",
    "    Solution: use relatively large documents \n",
    "\n",
    "Inexact Correlation Between Importance and Expression\n",
    "    subtly: important words are not used frequently\n",
    "    verbosity: lots of words are not used, thereby making all words seem important\n",
    "    succinctness: fe words are used, making their count seem less important\n",
    "\n",
    "$Introduce advanced method$ \n",
    "*Sentiment Analysis:\n",
    "words have been manually placed into categories\n",
    "large overlap between categories\n",
    "accuracy between categories\n",
    "accuracy subject to interpretation\n",
    "\n",
    "*Latent Dirichlet Allocation:\n",
    "documents belong to groups that are characterized by topic, rather tham specific words\n",
    "topics are not specified, but deduced from Bayesian probability of term co-occurence\n",
    "Documents are considered by their probability of having been generated from a  mixture of topics\n",
    "therefore, two documents may be measured similar even is they share relatively few words. \n",
    "\n",
    "*LongShortTermMemory (LSTM): \n",
    "A type of neural  network that takes input both from current data and a memory element of the network\n",
    "the network state retains information from previous iterations until they are explicitly reset by new information\n",
    "useful for connecting pronouns to meaning(among other things)\n",
    "\n",
    "*Word Embedding:\n",
    "difference between 'word space' and 'meaning space'\n",
    "in tf-idf, the word king is just a numenric value in a single column, alternatively, king can be a location on a multi dimensional system:\n",
    "gender axis - King -> Queen\n",
    "parental axis - King -> Prince\n",
    "authority axis - King -> Subject\n",
    "\n",
    "*Word2Vec\n",
    "deep neural networks to convert from word vectors to meaning vectors(with numeric values)\n",
    "allows for vector representation in math\n",
    "\n",
    "*BERT and ELMo\n",
    "    %ELMo: Similar to Word2Vec, but accounts for homonyms ('river bank' vs. 'bank account')\n",
    "    %Bert: \"Bidirectional Encoder Representations from Transformers\"\n",
    "    similar to ELMo, but accounts for context ('running a race' vs 'running a company')\n",
    "\n",
    "TFIDF (Term Frequency Inverse Document Frequency)\n",
    "\n",
    "term frequency\n",
    "\n",
    "tf(term, document) = '#' of times a term appears\n",
    "\n",
    "df(term, corpus) = '#' of documents that contain a term / '#' of documents in the corpus\n",
    "\n",
    "idf(term, corpus) = log(1 / df(term, corpus))\n",
    "\n",
    "TF-IDF is an acronym for the product of two parts: the term frequency tf and what is called the inverse document frequency idf. The term frequency is just the counts in a term frequency vector.\n",
    "\n",
    "tf-idf = tf(term,document) * idf(term,corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use .fit_transform for training data and .fit for test data"
   ]
  },
  {
   "source": [
    "### sklearn "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate tf and tf-idf matrices   \n",
    "tf-idf in sklearn \n",
    "\n",
    "1) sklearn.feature _extraction.text.CountVectorizer\n",
    "    + lots of built in operations like lowercase, tokenizer, stop_words, and N-grams\n",
    "    + lots of options like min and max document frequencies, domain-specific vocabulary, and max features\n",
    "    + output options(sparse matrix) Binary, Token count, Vocabulary_\n",
    "\n",
    "2) sklearn.feature_extraction.text.TfidfVectorizer\n",
    "    + lots of built in operations like lowercase, tokenizer, stop_words, and N-grams\n",
    "    + lots of options like min and max document frequencies, domain-specific vocabulary, and max features\n",
    "    + output options(sparse matrix) Binary, TTF-IDF, Vocabulary_\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf = CountVectorizer( )  # Check out the options!\n",
    "\n",
    "document_tf_matrix = tf.fit_transform(corpus).todense()\n",
    "\n",
    "pd.DataFrame(document_tf_matrix, columns = sorted(tf.vocabulary_))\n",
    "\n",
    "from math import log\n",
    "\n",
    "def idf(frequency_matrix):\n",
    "    df =  float(len(document_tf_matrix)) / sum(frequency_matrix > 0)\n",
    "    return [log(i) for i in df.getA()[0]]\n",
    "# print(sorted(tf.vocabulary_))\n",
    "# print(idf(document_tf_matrix))\n",
    "\n",
    "pd.DataFrame([idf(document_tf_matrix)], columns = sorted(tf.vocabulary_) )\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()  # Checkout out the options!\n",
    "\n",
    "document_tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "\n",
    "tfidf_df = pd.DataFrame(document_tfidf_matrix.todense(), columns = sorted(tfidf.vocabulary_))\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'TfidfVectorizer' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-53835add7a68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dogs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'like'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'dogs'\u001b[0m \u001b[0;34m'more'\u001b[0m \u001b[0;34m'than'\u001b[0m \u001b[0;34m'cats'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'TfidfVectorizer' object is not callable"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer() \n",
    "v = tfidf('Dogs', 'like' , 'dogs' 'more' 'than' 'cats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# term occurence = counting distinct words in each bag\n",
    "term_occ = list(map(lambda bow : Counter(bow), bows))\n",
    "\n",
    "# term frequency = occurences over length of bag\n",
    "term_freq = list()\n",
    "for i in range(len(docs)):\n",
    "    term_freq.append( {k: (v / float(len(bows[i])))\n",
    "                       for k, v in term_occ[i].items()} )\n",
    "\n",
    "# document occurence = number of documents having this word\n",
    "# term frequency = occurences over length of bag\n",
    "\n",
    "doc_occ = Counter( [word for bow in bows for word in set(bow)] )\n",
    "\n",
    "# document frequency = occurences over length of corpus\n",
    "doc_freq = {k: (v / float(len(docs)))\n",
    "            for k, v in doc_occ.items()}\n",
    "\n",
    "\n",
    "# the minimum document frequency (in proportion of the length of the corpus)\n",
    "min_df = 0.3\n",
    "\n",
    "# filtering items to obtain the vocabulary\n",
    "vocabulary = [ k for k,v in doc_freq.items() if v >= min_df ]\n",
    "\n",
    "\n",
    "# create a dense matrix of vectors for each document\n",
    "# each vector has the length of the vocabulary\n",
    "vectors = np.zeros((len(docs),len(vocabulary)))\n",
    "\n",
    "# fill these vectors with tf-idf values\n",
    "for i in range(len(docs)):\n",
    "    for j in range(len(vocabulary)):\n",
    "        term     = vocabulary[j]\n",
    "        term_tf  = term_freq[i].get(term, 0.0)   # 0.0 if term not found in doc\n",
    "        term_idf = np.log(1 + 1 / doc_freq[term]) # smooth formula\n",
    "        vectors[i,j] = term_tf * term_idf"
   ]
  },
  {
   "source": [
    "# NAIVE Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes Formula\n",
    "\n",
    "            P(B|A) P(A)\n",
    "P(A|B) =  ______________\n",
    "                P(B)\n",
    "where A and B are events and P(B) != 0\n",
    "\n",
    "\n",
    "marginal probability - observing A and B independently of each other. \n",
    "\n",
    "conditional probability - likelihood of event A occuring given B is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add formula\n",
    "\n",
    "\n",
    "Bayes rule - probability of an event, based on prior knowledge of conditions that might be related to the event. \n",
    "\n",
    "marginal probability - observing A and B independently of each other. \n",
    "\n",
    "conditional probability - likelihood of event A occuring given B is true.\n",
    "\n",
    "important: due to the bag of words featurization of text, where all features are assumed independent (naive), the input feture matrix is often very wide (~10000, 50000) and can be even greater that the number of data samples. Naive Bayes computationally efficient (is just sums). \n",
    "\n",
    "uses: classifying emails (spam or not), classifying news articles into genres, sentiment analysis (pos or neg review)\n",
    "\n",
    "Naive Bayes Classifier - calculate the probability that a document belongs to each class and choose the class with the highest probability. To do this calculate:\n",
    "*Priors: the probability that a generic document belongs to each class \n",
    "\n",
    "P(y_c) = num of articles in class c / total num of articles\n",
    "ex 8 articles 3 Sports, 4 politics, 1 arts\n",
    "sports = 3/8 = 0.375\n",
    "politics = 4/8 = 0.5\n",
    "arts = 1/8 = 0.125\n",
    "\n",
    "*Conditional Probabilities(Likelihoods): the probability that each word appears in each class. \n",
    "\n",
    "P(w_i | y_c) = num pf times w_i appears in articles of class y_c / total num of words in articles of class y_c\n",
    "\n",
    "ex P(sports) * 'ball' / word count of sports articles\n",
    "\n",
    "maximum likelihood estimation: \n",
    "\n",
    "laplace smoothing = the word has never appeared before in a document of the class. The prob would be 0. in this situation we add 1 to the numerator and number of words in the vocabulary  to denominator. \n",
    "\n",
    "preventing numerical underflow : \n",
    "take the log of both sides. \n",
    "\n",
    "ways to compare documents - \n",
    "    Euclidean distance\n",
    "\n",
    "\n",
    "\n",
    "Naive Bayes Pros and Cons:\n",
    "\n",
    "*Pros\n",
    "    good with wide data (p >> n)\n",
    "    good if n is small or n is quite big\n",
    "    fast to train (it's just counting)\n",
    "    good at online learning, streaming data, learns by processing one datapoint at a time\n",
    "    simple to implement, not necessarily memory-bound (DB implementations)\n",
    "    multi-class classification\n",
    "*Cons\n",
    "    naive assumption means correlated features are not actually trated right\n",
    "    sometimes outperformed bu other models\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Clustering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* What is the difference between \n",
    "**supervised** \n",
    "try to predict labels and target values. \n",
    "    linear and logistic regression with lasso or ridge regularization\n",
    "    decision trees, bagging, random forest, boosting\n",
    "    KNN\n",
    "Label == target == endogenous variable == dependent variable == y\n",
    "\n",
    "**unsupervised** learning\n",
    "no labels. no target\n",
    "    reasons for use:\n",
    "    EDA\n",
    "    Discovering latent variables\n",
    "    feature engineering \n",
    "    preprocessing\n",
    "    autoencoding - for making smaller approximations/representations of the original data as in data compression\n",
    "\n",
    "Clustering:\n",
    "●Data Exists\n",
    "●Distributed Across Multiple Dimensions\n",
    "●Reason to believe (or suspect) that difference exist in the data that have not been explicitly labeled\n",
    "●Reason to believe (or suspect) that knowing these differences helps make decisions, or improve outcomes\n",
    "\n",
    "a cluster is good when the points are near to each other and far from points outside the cluster. \n",
    "minimize \"within cluster variance\"\n",
    "\n",
    "clustering great for EDA, data splitting, nd feature generation. \n",
    "\n",
    "\n",
    "goal is to divide the data into distinct subgroups\n",
    "\n",
    "K-means ivolves repeatedly assigning points to clusters and then finding new clusters based on those points. \n",
    "\n",
    "* What are the steps of the **k-means** clustering algorithm?\n",
    "    choose a number of clusters k\n",
    "    randomly assign each point to a cluster\n",
    "    repeat:\n",
    "        a. for each of k clusters, compute cluster centroid by taking mean vector of points in cluster.\n",
    "        b. assign each data point to cluster for which centroid is closest (Euclidean 'L2')\n",
    "\n",
    "repeat until one of these is met:\n",
    " 1) specified number of iterations.\n",
    " 2) centroids do not change at all/no change to clusters\n",
    " 3) centroids don't move by much\n",
    " \n",
    "note: kmeans calculates a local median which is sometimes a bad one. An alternative would be to choose centroids based on random points to keep centroids farther apart. \n",
    "\n",
    "kmeans ++ \n",
    "    Choose one point for first center.\n",
    "    Repeat:\n",
    "        Calculate distance from each point to the nearest center $d_i$\n",
    "        Choose a point to be the next center, randomly, using a weighed probability $d_i^2$\n",
    " ... until k centers have been chosen.\n",
    "\n",
    "\n",
    "Non-Deterministic\n",
    "●Due to random initialization, it is not assured that points will always be assigned to the same cluster\n",
    "●Especially if clusters are not well separated or wrong value of k is chosen\n",
    "●May be necessary to repeat clustering multiple times\n",
    "\n",
    "* How does one choose the best k? \n",
    "there are several methods. \n",
    "    The Elbow Method\n",
    "    Silhouette Score\n",
    "    GAP Statistics\n",
    "(within-cluster sum of squares mentioned. problem is more clusters ==> lower WCSS )\n",
    "\n",
    "* What is the **elbow method**? \n",
    "  plot within-cluster-sum of squares and try to see what looks like an elbow. Sometimes there may not be a clear elbow to choose from. \n",
    "\n",
    "**silhouette score**?\n",
    "    measures how similar a point is to its own cluster compared to other clusters. \n",
    "    \n",
    "    for each point x_i:\n",
    "        a_i is the mean distance between x_i and all other points in the same cluster. \n",
    "        b_i is the mean distance between x_i and all points in the nearest cluster.\n",
    "\n",
    "        silhouette(x_i) = (b_i - a_i) / max(a_i,b_i)\n",
    "\n",
    "    the silhouette score is the average of silhouette score of all points. the higher the score, the tighter and more separated the clusters.\n",
    "\n",
    "possible scores:\n",
    "    near 1: very small tight cluster\n",
    "    0 : at the edge of two clusters, could be in either\n",
    "    < 0: oops.\n",
    "\n",
    "Assumptions\n",
    "●Correct k \n",
    "●Equal Variance\n",
    "●Isotropic Shape\n",
    "●Clusters do not need same number of points\n",
    "\n",
    "* How does the **curse of dimensionality** affect clustering?\n",
    "random variation in extra dimensions can hide significant differences between clusters. the more dimensions there are, the worse the problem.\n",
    "\n",
    "hierachical clustering:\n",
    "    1)assign each point to its own cluster\n",
    "    2)Repeat:\n",
    "        compute distences between clusters\n",
    "        merge closest clusters\n",
    "     ...until all are merged\n",
    "\n",
    "\n",
    "* What is the difference between k-means and **hierarchical clustering**?\n",
    "k-means specifically tries to put the data into the number of clusters you tell it to. \n",
    "\n",
    "in hierarchical clustering we do not declare a number of clusters. They are built agglomeratively, and we can choose whatever level wanted afterward. Just tells you, pairwise, what two things are most similar. \n",
    "\n",
    "Defining distance between clusters: Linkage\n",
    "    - between clusters, the difference is less clear then with points. There are a few methods we can use. \n",
    "    1) Single: minimum pairwise dissimilarity between points in cluters(not as good and can lead to long narrow clusters)\n",
    "    2) Complete: maximum pairwise dissimilarity between points in clusters (good)\n",
    "    3) Average: average of pairwise dissimilarity between points in clusters (also good)\n",
    "    4) Ward/Centroid: dissimilarity between centroids (used in genomics; risk of inversions)\n",
    "\n",
    "How to compare a point to clusters:\n",
    "    1) the average of each cluster (centroid)\n",
    "    2) closest point in each closter(single-linkage)\n",
    "    3) furthes point in each cluster(complete-linkage)\n",
    "    and others... \n",
    "DBSCAN (Density-Based Spacial Clustering of Applications with Noise) we don't specify the number of clusters. Instead we specify:\n",
    "    1) epsilon: distance between points for them to be connected\n",
    "    2) minPts: number of conected points for a point to be a 'core' pooint\n",
    "\n",
    "A cluster is all connected core points, plus others within epsilon of one of those. Other points are noise.\n",
    "\n",
    "\n",
    "Although simple generic prescriptions for choosing the individual attribute dissimilarities ... can be comforting, there is no substitute for careful thought in the context of each individual problem. Specifying an appropriate dissimilarity measure is far more important in obtaining success with clustering than the choice of clustering algorithm\n",
    "\n",
    "how to pick the best? \n",
    "if you believe you have a specific number of clusters, k means might be best, else hierarchical to determine how many clusters are needed. Additionaly, k means may be computationally faster if you have a large amount of data if the number of clusters is small. hierarchical is better at determining optimal number of clusters ans is more informative and interpretable than k-means. "
   ]
  },
  {
   "source": [
    "### Optional Content: Distribution-based clustering\n",
    "\n",
    "With distribution-based clustering we assume some fixed number of clusters, and assume they follow some (often normal) distribution. We then try to find the parameters that have the **maximum likelihood** of producing these data.\n",
    "\n",
    "This is more difficult then other problems we've seen because we don't know which point came from which distribution. We need to add some hidden variables to the problem: the probability each point came from each distribution. We can solve this by an **expectation-maximization** (EM) algorithm in which we alternate between expectation steps (where we calculate the hidden variables) and maximization steps (in which we calculate the maximum-likelihood parameters assuming the hidden variables are correct)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Principal-Component Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how it works PCA\n",
    "why to use it\n",
    "\n",
    "\n",
    "●PCA -> Principal Component Analysis\n",
    "three definitions:\n",
    "    1) PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated features into linearly uncorrelated features called principal components. \n",
    "\n",
    "    2) In PCA, the dataset is transformed from its original coordinate system to a new coordinate system. The new system is chosen by the data itself. The first new axis (a.k.a principal component) is chosen in the direction of the most variance of the data. The second axis is orthogonal to the first axis and is in the direction of the next most variance in the data. \n",
    "    More and more orthogoonal axes can be added to describe all the variance in the data. \n",
    "    Usually the majority of the variance of the data is contained in the first few proncipal components. so the rest of the axes can be ignored, reducing dimensionality in the data. \n",
    "\n",
    "    3) When faced with a large set of correlated variables, principal components allow us to summarize this set with a smaller number of representative variables that collectively explain most of the variability in the original set. \n",
    "    Principal component directions are directions in feature space along which the data are highly variable. \n",
    "    PCA refers to the process by which principal components are cmputed, and the use of these components in understanding the data. \n",
    "\n",
    "finds the major patterns in data for dimensionality reduction.\n",
    "\n",
    "PCA defines orthogonal axes of variance that can be used to describe the variance in the data. \n",
    "\n",
    "●Define PCA conceptually how does it work\n",
    "reduce the number of variables of a data set, while preserving as much information as possible. Creating principal components from the features that are in order of variance. \n",
    "\n",
    "The Principal Components have now got nothing to do with the original features. We will get 300 principal components from 300 features. Now here comes the beauty of PCA — The newly formed transformed feature set or the Principal Components will have the maximum variance explained in the first PC. The second PC will have the second highest variance and so on.\n",
    "\n",
    "For example, if the first PC explains 68% of the total variance in data, the 2nd feature explains 15% of the total variance and the next 4 features comprise of 14% variance in total. So you have 97% of the variance explained by just 6 Principal Components! \n",
    "\n",
    "\n",
    "●Describe motivation for PCA in machine learning\n",
    "\n",
    "PCA technique is particularly useful in processing data where multi-colinearity exists between the features/variables.\n",
    "\n",
    "PCA can be used when the dimensions of the input features are high (e.g. a lot of variables).\n",
    "\n",
    "PCA can be also used for denoising and data compression.\n",
    "This does two helpful things at once:\n",
    "    Dimensionality reduction\n",
    "    Remove collinearity of features\n",
    "With fewer dimensions you can:\n",
    "    Better visualize your data\n",
    "    Make computations for your algorithms easier\n",
    "    Identify structure for supervised learning\n",
    "    \n",
    "Disadvantages to PCA: Principal components are linear combinations of the features and are less interpretable \n",
    "\n",
    "\n",
    "●Be able to apply PCA in numpy\n",
    "    ○Understand and apply PCA’s mathematical definition\n",
    "    \n",
    "    ○Review some statistical concepts\n",
    "\n",
    "●Provide a step-by-step how-to procedure for performing PCA (the eigendecomposition version)\n",
    "    Done two ways:\n",
    "        1) eigen-decomposition of the X covariance or correlation matrix\n",
    "        2) singular value decomposition\n",
    "    \n",
    "    Steps(eigen-decomposition):\n",
    "        1) create 'design matrix' X by mean-centering and (usually) by dividing by the standard deviation. (this is standardizing the columns)\n",
    "        2) compute the covariance(if only mean-centered) or correlation matrix:\n",
    "                (1/N)*(X^T)*X\n",
    "        3)find the eigenvectors (v) and eigenvalues (lambda) of the covariance/correlation matrix (A):\n",
    "        A = (1/N)*(X^T)*X    Av = (lambda)v\n",
    "the eigenvectors are the proncipal components. \n",
    "\n",
    "another explanation of how to:\n",
    "1) Standardize columns\n",
    "2) Create covariance (correlation if standardized) matrix\n",
    "3) Find the eigenvectors and eigenvalues of the covariance/correlation matrix\n",
    "4) The eigenvectors are the principal components\n",
    "\n",
    "\n",
    "\n",
    "●Provide a methodology for picking the number of principal components\n",
    "\n",
    "An n x p matrix (n rows, p columns) has min(n-1, p) principal components, but we’re usually not interested in them.\n",
    "\n",
    "Just want the “first few” that “capture most” of the variance.\n",
    "\n",
    "“Capture most” is usually about 90%.\n",
    "\n",
    "The eigenvalues are measures of variance.\n",
    "\n",
    "The cumulative sum of the eigenvalues up to and including a certain principal component, divided by the sum of all the eigenvalues, is the explained variance.\n",
    "\n",
    "A scree plot is often used to visualize this.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "●Perform PCA in sklearn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Besides PCA, other dimensionality reduction techniques●LASSO regularization●“Relaxed” LASSO (next slide)●sklearn’s Feature Selection page○VarianceThreshold○SelectKBest○Recursive Feature Elimination (RFE)○SelectFromModel●Decision Tree Based○.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple linear dataset\n",
    "m = 2 # slope\n",
    "b = 0 # intercept\n",
    "x1 = np.random.uniform(-5, 5, size = 20)\n",
    "x2 = m * x1 + b\n",
    "\n",
    "# reshape to column vectors of arbitrary length\n",
    "x1 = x1.reshape((-1,1)) \n",
    "x2 = x2.reshape((-1,1))\n",
    "\n",
    "# make it into a matrix\n",
    "X = np.hstack((x1, x2))\n",
    "# define the rotation matrix, R\n",
    "# use -theta to rotate it back to the X1 axis\n",
    "R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "              [np.sin(theta), np.cos(theta)]])\n",
    "# rotate the data\n",
    "XR = np.dot(X, R)\n",
    "\n",
    "\n",
    "\n",
    "Using numpy\n",
    "\n",
    "# standardize using numpy\n",
    "X_std = (X - X.mean(axis=0))/X.std(axis=0)\n",
    "print(X_std) #ugh, so many digits\n",
    "print(np.around(X_std, 2)) # better\n",
    "\n",
    "# check that column means are 0, standard deviation of 1\n",
    "print(X_std.mean(axis=0))\n",
    "print(X_std.std(axis=0))\n",
    "\n",
    "\n",
    "N = X.shape[0]\n",
    "A = 1/(N)*np.dot(X_std.T, X_std)"
   ]
  },
  {
   "source": [
    "# SVD Singular Value Decomposition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem - large data - matrix of photos with pixels 500x500\n",
    "\n",
    "SVD - because \n",
    "\n",
    "\n",
    "principal component\n",
    "\n",
    "latent features (latent topic)\n",
    "\n",
    "latent vs explicit\n",
    "\n",
    "\n",
    "●Know why we need SVD\n",
    "covariance matrix can become very large as the number of components in feature space increases. \n",
    "\n",
    "SVD can help us to reduce the size of calculations (computational resources) by offering to use matrix X^T (instead of covariace matrix X^T*X) to find the same eigenvectors as PCA with eigenvalues which are the square of X^T*X eigenvalues. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6\n",
    "Singular Value Decomposition (SVD)\n",
    "OutlineBy the end of this lecture you:\n",
    "\n",
    "●Recap - PCA\n",
    "\n",
    "●Know why we need SVD\n",
    "\n",
    "●Apply SVD to reduce the feature space\n",
    "\n",
    "●Learn how to apply apply SVD for image compression\n",
    "\n",
    "●Learn how to apply SVD to audio\n",
    "\n",
    "Problem!Imagine we have 100 images and each image is 500*500 pixels. For this problem the size of our covariance matrix is 6.25*1010. If each pixel is shown with a double precision number (64bites) then the total needed memory is 4*1012 which means 4TB! This is very big for a small number of images! Is there any way to reduce the computational efforts without sacrificing the results? \n",
    "\n",
    "Why SVD?Covariance matrix can become very large as the number of components in feature space increases. SVD can help us to reduce the size of calculations (computational resources) by offering to use matrix XT (instead of covariance matrix XTX) to find the same eigenvectors as PCA and with eigenvalues which are the square of XTX eigenvalues. \n",
    "\n",
    "SVD decomposes matrix Am*n into three matrix in which matrix S is diagonal.\n",
    "Relation Between SVD and PCAIn SVD we decompose matrix X = USV. Calculating the covariance matrix XTX gives: XTX = (USV)T(USV) = VTSTUTUSV = VTSTSV   (Since UTU = 1)From PCA we know that XTX = ETDE where E is the eigenvectors matrix and D is the eigenvalue matrix. Comparing two equations we can conclude that E = V and and D = STS. Therefore the eigenvectors or PCA are the eigenvectors of SVD and since S and D are diagonal the eigenvalues of SVD are the square of PCA eigenvalues.  \n",
    "\n",
    "●Apply SVD to reduce the feature space\n",
    "\n",
    "●Learn how to apply apply SVD for image compression\n",
    "\n",
    "●Learn how to apply SVD to audio \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Topic Modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What is NMF?\n",
    "a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.\n",
    "\n",
    "\n",
    "Popular Applications\n",
    "\n",
    "    ●Soft Clustering:\n",
    "        –Each observation may have partial membership in           multiple clusters\n",
    "        –Genre Analysis\n",
    "    ●Computer Vision–Identifying/classifying based on key      components\n",
    "        –Compression\n",
    "        –Error correction/enhancement\n",
    "\n",
    "    ●Document clustering\n",
    "\n",
    "    ●Recommender Systems\n",
    "\n",
    "Document Clustering\n",
    "\n",
    "Recommender Systems\n",
    "\n",
    "Data Compression\n",
    "\n",
    "Methods\n",
    "    Alternating Least Squares\n",
    "    Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "non negative matrix factorization topic modeling  given starting code figure out latent topics\n",
    "\n",
    "\n",
    "issues with term frequency \n",
    "\n",
    "\n",
    "TF-IDF or ( Term Frequency(TF) — Inverse Dense Frequency(IDF) )is a technique which is used to find meaning of sentences consisting of words and cancels out the incapabilities of Bag of Words technique which is good for text classification or for helping a machine read words in numbers. However, it just blows up in your face when you ask it to understand the meaning of the sentence or the document.\n",
    "I highly suggest you read about BoW before you go through this article to get a context -\n",
    "\n",
    "So what is it, do you want to understand using an example ?\n",
    "\n",
    "Let’s say a machine is trying to understand meaning of this —\n",
    "Today is a beautiful day\n",
    "What do you focus on here but tell me as a human not a machine?\n",
    "This sentence talks about today, it also tells us that today is a beautiful day. The mood is happy/positive, anything else cowboy?\n",
    "Beauty is clearly the adjective word used here. From a BoW approach all words are broken into count and frequency with no preference to a word in particular, all words have same frequency here (1 in this case)and obviously there is no emphasis on beauty or positive mood by the machine.\n",
    "The words are just broken down and if we were talking about importance, ‘a’ is as important as ‘day’ or ‘beauty’.\n",
    "But is it really that ‘a’ tells you more about context of a sentence compared to ‘beauty’ ?\n",
    "No, that’s why Bag of words needed an upgrade.\n",
    "Also, another major drawback is say a document has 200 words, out of which ‘a’ comes 20 times, ‘the’ comes 15 times etc.\n",
    "Many words which are repeated again and again are given more importance in final feature building and we miss out on context of less repeated but important words like Rain, beauty, subway , names.\n",
    "So it’s easy to miss on what was meant by the writer if read by a machine and it presents a problem that TF-IDF solves, so now we know why do we use TF-IDF.\n",
    "\n",
    "\n",
    "\n",
    "nmf\n",
    "starter code - figure latent topics\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL joins and conditions \n",
    "is like or not like \n",
    "coelesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}