{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Machine Learning: process to discover a relationship between known inputs and known outputs, so that (reasonably) accurate predictions can be made for unknown inputs.    \n",
    "\n",
    "Key Assumptions:     \n",
    "there is a relationship between the inputs and outputs.   \n",
    "the relationship can be discovered by a computer.    \n",
    "relationship will contiunue to hold for the immediate future.    \n",
    "the most significant features are represented in the data.    \n",
    "the relevant features can be quantified or classified.     \n",
    "\n",
    "\n",
    "labels/targets: columns of the dataset\n",
    "examples using houses Bedrooms, Bathrooms, Square Feet (these are also X) - Matrix     \n",
    "Sales price (this is the y) - this is a vector      \n",
    "             \n",
    "Two types of supervised learning             \n",
    "Regression -  target is a number, Ex. sales price of a house, number of visitors to a museum, inches of snow in december, gas mileage of a car      \n",
    "\n",
    "Classification - target is a category, Ex. an email is or is not spam, paining was Picasso or Van Gogh, House will sell or not sale by a date, a person will vote Republican, Democrat, Libertarian, or Green       \n",
    "\n",
    "\n",
    "RSS =\n",
    "\n",
    "MSE = \n",
    "\n",
    "RMSD = \n",
    "\n",
    "TSS =  d\n",
    "\n",
    "\n",
    "Decision Boundary -   \n",
    "\n",
    "Threshold-     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## KNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Implement KNN algorithm     \n",
    "Training Algorithm:      \n",
    "1.store all the data\n",
    "Prediction Algorithm (predict the value of a new point x):    \n",
    "2.calculate the distance from x to all points in dataset\n",
    "3.sort the points in your dataset by increasing distance from x\n",
    "4.predict the majority label of the k closest points\n",
    "\n",
    "Explain the difference between KNN for regression vs. classification:   \n",
    "regression predicts the average label amongst k closest neighbors (mean)    \n",
    "classification predicts the most common label amongst its k closest neighbors      \n",
    "    \n",
    "Understand KNN hyperparameters\n",
    "a parameter whose value is used to control the learning process\n",
    "i.e. distance metric and k     \n",
    "      \n",
    "How does changing them affect the model?\n",
    "high k can lead to underfitting       \n",
    "low k can lead to overfitting      \n",
    "\n",
    "Describe the curse of dimensionality   \n",
    "with high dimensions the nearest neighbors can be very far away     \n",
    "remedies - get more data, get more varied data, reduce number of dimensions,reduce relative importance of csome data features, standardize data. \n",
    "\n",
    "When dealing with models that measure distance it is important to scale the data so that it is in the same format. \n",
    "\n",
    "mean squared error\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Pros: super simple, training is trivial, works with any number classes, easy to add more data, few hyperparameters  \n",
    "          \n",
    "Cons: high prediction cost(especially large datasets), bad with high dimensions, categorical features don't work well   \n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Bias-Variance Trade Off"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "zeroth order polynomial (mean), line is more flexible as order increased but get wiggly as order becomes very large. \n",
    "              \n",
    "When order is low:    \n",
    "errors are relatively large     \n",
    "direction varies by region    \n",
    "magnitude of errors varies\n",
    "\n",
    "When order is increased:     \n",
    "error decreases drastically     \n",
    "still evidence of error by blocks   \n",
    "\n",
    "●High Bias:      \n",
    "○model is underfit     \n",
    "○Line is too rigid     \n",
    "○Not enough features      \n",
    "○Errors tend towards one side or the other in blocks    \n",
    "○Error magnitude not randomly distributed     \n",
    "       \n",
    "\n",
    "●High Variance:         \n",
    "○Model is overfit         \n",
    "○Line is too flexible        \n",
    "○Too many features         \n",
    "○Errors tend to alternate positive/negative○Error magnitudes normally distributed     \n",
    "         \n",
    "Conclusion:      \n",
    "●Optimal model has minimum total error    \n",
    "●Neither overfit nor underfit      \n",
    "●It is necessary to do train-test split to observe error on unseen data     \n",
    "●All else being equal, prefer simpler models to more complex   \n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Cross Validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cross validation is a model validation technique for assessing how the results of a statustical analysis will generalize to an independant data set.  \n",
    "         \n",
    "Describe why cross validation is used:\n",
    "1) attempting to quantify how well a model (of some given complexity) will predict an unseen data set    \n",
    "2) tuning hyperparameters of models to get best predictions      \n",
    "                 \n",
    "Describe how to do it (especially k-fold cross validation)\n",
    "1) Split your data (after splitting out hold-out set) into training/validation sets - commonly 70/30, 80/20, 90/10         \n",
    "2) use training set to train several models of varying comlexity\n",
    "3) evaluate each model using validation set   \n",
    "4) keep the model that performs best over the validation set   \n",
    "\n",
    "two types - single train-test split\n",
    "and k-fold which involves running k amount of calculations    \n",
    "    \n",
    "Note:   \n",
    "People use different terms for the splits    \n",
    "All data -> Train, Test, Hold-out       \n",
    "All data -> Train, Validate, Test      \n",
    "All data -> Train, Validate, Hold-out       \n",
    "All the same idea.          \n",
    "        \n",
    "if overfitting:    \n",
    "Get more data - get more data if possible     \n",
    "keep only a suSubset selection - bset of your predictors       \n",
    "Regularization- restrict models parameter space     \n",
    "Dimensionaliy Reduction - project data into a lower dimensional space   \n",
    "\n",
    "\n",
    "\n",
    "●In the individual assignment, code it from scratch\n",
    "\n",
    "●Aside: introduce feature selection/elimination\n",
    "\n",
    "●Contrast cross validation with other statistical model comparison methods\n"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Linear Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "●Define \n",
    "supervised learning      \n",
    "learning a function that maps an input to an output based on input-output pairs (if has a y culumn)    \n",
    "\n",
    "unsupervised learning   \n",
    "a type of self organized learning that helps find previously unknown pattern in data set without pre-existing labels (no y column)  \n",
    "\n",
    "parametric algorithm\n",
    "fixed number of parameters, makes assumption about the structure of the data, works well if assumptions are correct   \n",
    "Ex. linear regression, neural networks, statistical distribution defined by a finite set of parameters    \n",
    "\n",
    "non-parametric algorithm\n",
    "uses a flexible number of parameters that grow in number as it learns from the data, makes fewer assumptions about the data      \n",
    "Ex. K-nearest neighbors, decision trees     \n",
    "   \n",
    "●Define linear regression\n",
    "betas - parameters/values/weights\n",
    "given a dataset oof n statistical units,a linear regression model assumes that the relationship between the dependent variable y and the p-vector of regressors x is linear     attempts to model the relationship between two variables by fitting a linear equation to observed data.      \n",
    "\n",
    "\n",
    "○Difference between simple and multiple linear regression\n",
    "simple linear regression has only one predictor, multiple linear regression has more than one predictor   \n",
    "       \n",
    "●Interpret linear regression coefficients\n",
    "the beta, parameter/coefficient - what we need to figure out from fitting the data      \n",
    "        \n",
    "●Describe how coefficients in linear regression are determined  \n",
    "mathematical optimization,      \n",
    "\n",
    "●Assess the accuracy of the coefficients\n",
    "\n",
    "●Assess the accuracy of a linear regression model\n",
    "RSS = sum(y1 - yhat1)**2  residual sum of squares  = ok, but depends on n      \n",
    "MSE = mean squared error - better but in squared units of response    \n",
    "RMSD = Root mean square error(or deviation) - in units of response  \n",
    "R**2 = TSS-RSS/TSS = 1- RSS/TSS - R squared, ranges from 0 to 1 (but doesnt penalize for model comlexity)    \n",
    "Rbar**2 = adjusted R-squared (0 to 1), penalizes for model complexity according to number of predictors   \n",
    "\n",
    "●Interpret an Ordinary Least Squares linear regression summary from Statsmodels\n",
    "\n",
    "There were hypothesis tests checking if coefficients in the model were significant (not zero).  \n",
    "     \n",
    "The F-statistic is a hypothesis test where the Null Hypothesis is that all the coefficients are = 0 (so model is not useful).  \n",
    "      \n",
    "The alternative is that at least one of the coefficients is not 0.   \n",
    "   \n",
    "For no relationship between predictors and response (H0), F statistic is near 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RSS = sum(y1 - yhat1)**2  residual sum of squares  \n",
    "MSE = mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Inferential Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "●Understand the differences between predictive and inferential linear regression   \n",
    "               \n",
    "Predictive linear regression\n",
    "goal - accurately predict a target; model picked by trying different features, using cross validation. We care that it predicts well on unseen data.       \n",
    "     \n",
    "Inferential linear regression     \n",
    "goal - learn something accurate about the process that made the data. Infer(estimate) coefficients; model based on trying different features and checking residuals to see if we are violating some of the assumptions of linear regression. We care the estimates are accurate and valid.\n",
    "   \n",
    "●Identify the assumptions of an inferential linear regression model  \n",
    "   \n",
    "●Define and detect collinearity between features using VIF\n",
    "\n",
    "●Understand what confounding is and how it can impact a model’s results  \n",
    "   \n",
    "●Encode categorical features to use in a regression model   \n",
    "   \n",
    "●Interpret the model output for a generalized audience    \n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# at slide 5"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anscombe's quartet comprises four data sets that have nearly identical simple descriptive statistics, yet have very different distributions and appear very different when graphed. Each dataset consists of eleven (x,y) points. They were constructed in 1973 by the statistician Francis Anscombe to demonstrate both the importance of graphing data before analyzing it and the effect of outliers and other influential observations on statistical properties. He described the article as being intended to counter the impression among statisticians that \"numerical calculations are exact, but graphs are rough.\"[1] It has been rendered as an actual musical quartet.[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance of the errors is not contstant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Computational Complexity and Big-O Notation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List the benefits of being able to calculte the efficiency of a particular algorithm         \n",
    "     \n",
    "Use Big-O notation for an given algorithm       \n",
    "mathematical notation to describe limiting behavior      \n",
    "Describe what worst, best, and average cases are       \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds - wins / losses\n",
    "probability - wins / (wins + losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Decision Rules\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}